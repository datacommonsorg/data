# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""cli to diff prod data and dev data generated by import automation tool.

This command does the following:
1) Trigger a set of imports on both live and dev instances
2) Wait
3) Compare the file hashes of what is generated from the trigger.

If there are no diffs, nothing is printed :D

Usage example (take the diffs of 3 imports):

cd import-automation/executor

PYTHONPATH=$(pwd) python commands/diff_imports.py \
    --abs_import_names=scripts/covid19_india/cases_count_states_data:covid19IndiaCasesCountStatesData,scripts/covid19_india/medical_tests_in_data:covid19IndiaMedicalTestsInData,scripts/ourworldindata/covid19:OWID_COVID19
"""
from dataclasses import dataclass
import time
from typing import Dict

from absl import flags
from absl import app
from google.cloud import storage

from app.executor.cloud_scheduler import trigger_job

_IMPORT_NAME_FORM = '''
<path to the directory containing the manifest>:<import name>
'''.strip()

FLAGS = flags.FLAGS
flags.DEFINE_list(name='abs_import_names',
                  default=None,
                  help=('Comma separated list of absolute import names '
                        'to run the diff on. Follows the format: '
                        f'{_IMPORT_NAME_FORM}.'))

flags.DEFINE_string(name='executor_type',
                    default="GKE",
                    help=('One of GKE, GAE'))

flags.DEFINE_string(name='prod_bucket',
                    default="datcom-prod-imports",
                    help=('GCS bucket name of live data'))

flags.DEFINE_string(name='prod_bucket_project_id',
                    default="datcom-204919",
                    help=('GCP project id for prod_bucket'))

flags.DEFINE_string(name='dev_bucket',
                    default="datcom-dev-imports",
                    help=('GCS bucket name of test data'))

flags.DEFINE_string(name='dev_bucket_project_id',
                    default="google.com:datcom-store-dev",
                    help=('GCP project id for dev_bucket'))


def _get_file_hashes(abs_import_name, bucket_project_id,
                     bucket_name: str) -> Dict[str, str]:
    """Returns a mapping of filepaths to md5 hash of the contents of latest folder."""
    parts = abs_import_name.split(':')
    if len(parts) != 2:
        raise Exception('Invalid absolute import name %s' % abs_import_name)
    path_from_script = '/'.join(parts)

    client = storage.Client(project=bucket_project_id)
    bucket = client.bucket(bucket_name)
    version_file = f'{path_from_script}/latest_version.txt'
    latest_version = bucket.blob(version_file).download_as_string().decode()

    file_hashes = {}
    for blob in client.list_blobs(
            bucket_name, prefix=f'{path_from_script}/{latest_version}'):
        # <path>/<version>/<file> -> <path>/<file>
        canonical_name = blob.name.replace(latest_version + '/', '')
        file_hashes[canonical_name] = blob.md5_hash
    return file_hashes


def _diff_file_hashes(abs_import_name, prod_project_id, prod_bucket,
                      dev_project_id, dev_bucket: str):
    """Returns error if there is diff of import between base_bucket and head_bucket."""
    prod_file_hashes = _get_file_hashes(abs_import_name, prod_project_id,
                                        prod_bucket)
    dev_file_hashes = _get_file_hashes(abs_import_name, dev_project_id,
                                       dev_bucket)

    has_diff, missing, unexpected = [], [], []

    for name, hash in prod_file_hashes.items():
        if name not in dev_file_hashes:
            missing.append(name)
        elif hash != dev_file_hashes[name]:
            has_diff.append(name)

    for name in dev_file_hashes:
        if name not in prod_file_hashes:
            unexpected.append(name)

    return has_diff, missing, unexpected


def main(argv):
    for abs_import_name in FLAGS.abs_import_names:
        trigger_job(
            'google.com:datcom-store-dev',
            'us-central1',
            abs_import_name,
            f'_{FLAGS.executor_type}'  # job id suffix
        )

        trigger_job(
            'google.com:datcom-data',
            'us-central1',
            abs_import_name,
            ''  # Legacy GAE has no job suffix
        )

    # There is no way to know when a job triggered by Cloud Scheduler finishes.
    # Since the timeout is 30min, we sleep for 30min + some buffer time to wait
    # for all jobs to finish.
    # If you know that the imports being diffed takes much less time, feel
    # free to adjust the time below.
    print('Waiting for jobs to complete')
    time.sleep(60 * 35)  # 35 minutes
    print('Done for jobs to complete')

    for abs_import_name in FLAGS.abs_import_names:
        has_diff, missing, unexpected = _diff_file_hashes(
            abs_import_name, FLAGS.prod_bucket_project_id, FLAGS.prod_bucket,
            FLAGS.dev_bucket_project_id, FLAGS.dev_bucket)

        for name in has_diff:
            print(f'Content diff found in {name}')

        for name in missing:
            print(f'Missing file in the dev version: {name}')

        for name in unexpected:
            print(f'Got unexpected file not found in live version: {name}')


if __name__ == '__main__':
    app.run(main)
