main:
  params: [args]
  steps:
    - init:
        assign:
          - lock_timeout: 86400 # 24 hours
          - wait_period: 300 # seconds
          - project_id: '${sys.get_env("PROJECT_ID")}'
          - dataflow_job_name: 'ingestion-job'
          - dataflow_gcs_path: 'gs://datcom-templates/templates/flex/ingestion.json'
          - location: '${sys.get_env("LOCATION")}'
          - spanner_database_id: '${sys.get_env("SPANNER_DATABASE_ID")}'
          - ingestion_function: ${"https://" + location + "-" + project_id + ".cloudfunctions.net/" + "spanner-ingestion-helper"}
          - aggregation_function: ${"https://" + location + "-" + project_id + ".cloudfunctions.net/" + "import-aggregation-helper"}
          - import_list: ${default(map.get(args, "imports"), [])}
          - execution_error: null
    - acquire_ingestion_lock:
        call: http.post
        args:
          url: ${ingestion_function}
          auth:
            type: OIDC
          body:
            actionType: acquire_ingestion_lock
            workflowId: '${sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID")}'
            timeout: ${lock_timeout}
        result: lock_status
    - process_ingestion:
        try:
          steps:
            - get_import_list:
                call: http.post
                args:
                  url: ${ingestion_function}
                  auth:
                    type: OIDC
                  body:
                    actionType: get_import_list
                    importList: ${import_list}
                result: import_info
            - run_ingestion_job:
                call: run_dataflow_job
                args:
                  import_list: '${json.encode_to_string(import_info.body)}'
                  project_id: ${project_id}
                  job_name: ${dataflow_job_name}
                  template_gcs_path: ${dataflow_gcs_path}
                  location: ${location}
                  spanner_database_id: ${spanner_database_id}
                  wait_period: ${wait_period}
                result: dataflow_job_id
            - run_aggregation:
                call: http.post
                args:
                  url: ${aggregation_function}
                  auth:
                    type: OIDC
                  body:
                    importList: ${import_info.body}
            - update_ingestion_status:
                call: http.post
                args:
                  url: ${ingestion_function}
                  auth:
                    type: OIDC
                  body:
                    actionType: update_ingestion_status
                    workflowId: '${sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID")}'
                    jobId: '${dataflow_job_id}'
                    importList: '${import_info.body}'
                result: function_response
        except:
          as: e
          steps:
            - capture_error:
                assign:
                  - execution_error: ${e}
    - release_ingestion_lock:
        call: http.post
        args:
          url: ${ingestion_function}
          auth:
            type: OIDC
          body:
            actionType: release_ingestion_lock
            workflowId: '${sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID")}'
        result: function_response
    - check_error:
        switch:
          - condition: ${execution_error != null}
            raise: ${execution_error}
    - return_import_info:
        return: '${import_info.body}'

# This sub-workflow launches a Dataflow job and waits for it to complete.
run_dataflow_job:
  params: [import_list, project_id, job_name, template_gcs_path, location, spanner_database_id, wait_period]
  steps:
    - init:
        assign:
          - jobName: '${job_name + "-" + string(int(sys.now()))}'
    - log_imports:
        call: sys.log
        args:
          text: '${"Dataflow job: " + jobName + " Import list: " + import_list}'
          severity: INFO 
    - check_if_empty:
        switch:
          - condition: ${import_list == "[]"}
            return: ''
    - launch_dataflow_job:
        call: googleapis.dataflow.v1b3.projects.locations.flexTemplates.launch
        args:
          projectId: '${project_id}'
          location: '${location}'
          body:
            launchParameter:
              containerSpecGcsPath: '${template_gcs_path}'
              jobName: '${jobName}'
              parameters:
                importList: '${import_list}'
                spannerDatabaseId: '${spanner_database_id}'
              environment:
                numWorkers: 3
                machineType: 'n2-highmem-8'
        result: launch_result
    - wait_for_job_completion:
        call: sys.sleep
        args:
          seconds: ${wait_period}
        next: check_job_status
    - check_job_status:
        call: googleapis.dataflow.v1b3.projects.locations.jobs.get
        args:
          projectId: '${project_id}'
          location: '${location}'
          jobId: '${launch_result.job.id}'
          view: 'JOB_VIEW_SUMMARY'
        result: job_status
        next: check_if_done
    - check_if_done:
        switch:
          - condition: ${job_status.currentState == "JOB_STATE_DONE"}
            return: ${launch_result.job.id}
          - condition: ${job_status.currentState == "JOB_STATE_FAILED" or
              job_status.currentState == "JOB_STATE_CANCELLED"}
            next: fail_workflow
        next: wait_for_job_completion
    - fail_workflow:
        raise:
          message: '${jobName + " dataflow job failed with status: " + job_status.currentState}'
          code: 500