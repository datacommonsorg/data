main:
  params: [args]
  steps:
    - init:
        assign:
          - projectId: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - region: ${sys.get_env("LOCATION")}
          - imageUri: "gcr.io/datcom-ci/dc-import-executor:stable"
          - jobId: ${text.substring(args.jobName, 0, 50) + "-" + string(int(sys.now()))}
          - importName: ${args.importName}
          - importConfig: ${args.importConfig}
          - gcsMountBucket: ${sys.get_env("GCS_MOUNT_BUCKET")}
          - gcsImportBucket: ${sys.get_env("GCS_BUCKET_ID")}
          - gcsMountPath: "/tmp/gcs"
          - ingestionHelper: "spanner-ingestion-helper" 
          - functionUrl: ${"https://" + region + "-" + projectId + ".cloudfunctions.net/" + ingestionHelper}
          - startTime: ${sys.now()}
    - runImportJob:
        try:
          call: googleapis.batch.v1.projects.locations.jobs.create
          args:
            parent: ${"projects/" + projectId + "/locations/" + region}
            jobId: ${jobId}
            body:
              allocationPolicy:
                instances:
                - policy:
                    machineType: ${args.resources.machine}
                    provisioningModel: "STANDARD"
                    bootDisk:
                      image: "projects/debian-cloud/global/images/family/debian-12"
                      size_gb: ${args.resources.disk}
                  installOpsAgent: true
              taskGroups:
                taskSpec:
                  volumes:
                    - gcs:
                        remotePath: ${gcsMountBucket}
                      mountPath: ${gcsMountPath}
                  computeResource:
                    cpuMilli: ${args.resources.cpu}
                    memoryMib: ${args.resources.memory}
                  runnables:
                    - container:
                        imageUri: ${imageUri}
                        commands:
                        - ${"--import_name=" + args.importName}
                        - ${"--import_config=" + args.importConfig}
                      environment:
                        variables:
                          IMPORT_NAME: ${importName}
                          BATCH_JOB_NAME: ${jobId}
                taskCount: 1
                parallelism: 1
              logsPolicy:
                destination: CLOUD_LOGGING
            connector_params:
              timeout: 604800 #7 days
              polling_policy:
                initial_delay: 60
                multiplier: 2
                max_delay: 600
          result: importJobResponse
        except:
          as: e
          steps:
            - updateImportStatus:
                call: http.post
                args:
                  url: ${functionUrl}
                  auth:
                    type: OIDC
                  body:
                    actionType: 'update_import_status'
                    jobId: ${jobId}
                    importName: ${text.split(args.importName, ":")[1]}
                    status: 'FAILED'
                    duration: ${int(sys.now() - startTime)}
                    version: ${"gs://" + gcsImportBucket + "/" + text.replace_all(args.importName, ":", "/")}
                    schedule: ${default(map.get(args, "schedule"), "")}
                result: functionResponse
            - failWorkflow:
                raise: ${e}
    - updateImportVersion:
        call: http.post
        args:
          url: ${functionUrl}
          auth:
            type: OIDC
          body:
            actionType: 'update_import_version'
            importName: ${args.importName}
            version: 'staging'
            reason: 'import-automation'
        result: functionResponse
    - returnResult:
        return:
          jobId: ${jobId}
          importName: ${importName}