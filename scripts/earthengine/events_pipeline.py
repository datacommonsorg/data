# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the 'License');
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#         https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an 'AS IS' BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Script to generate events from source data.
The pipeline to generate events can have the following supported stages:
  1. earthengine: Extract an image from Google EarthEngine as a geoTif
       on Google Cloud Storage (GCS)
  2. download: Download data from source, incrementally with
       any source specific processing and rate limits
  3. bq_export: Export a BigQuery table using an SQL query into a CSV
       on Google Cloud Storage (GCS)
  3. raster_csv: Process source data downloaded from one of the above steps
        into CSV with S2 cells.
  4. events: Generate events and related stats from the csv data.

The stages are run sequentially in the order listed in the config,
with each stage run with the configs for that stage.
The output files generated by one stage is passed on to the next as inputs.
In case of an error in a stage, the stage runner is expected to terminate
the script.

The whole pipeline can be restarted to regenerate the data either on failure or
to refresh with new data from source.
Some stages, such as 'download' can resume from past runs and incrementally
process new data.  Some stages, like 'events', process all files in an
input folder when re-run.

To use it, create a config file with a python dictionary with all the settings
for the stages to be run in sequence.

The config value can have references to other settings with python format
syntax, '{<config>}'. The values are resolved when that setting is used.

  {
    'defaults': {
      # Default settings for all stages.
      'gcs_bucket': 'my-bucket',
      'gcs_folder': 'my-import-name',
    },
    # Stage specific settings
    'stages': [
      {
        'stage': 'download',
        'output_dir': 'gs://{gcs_bucket}/{gcs_folder}/{stage}/{year}',
        'url': 'http://my-data-source/',
      },
      {
        'stage': 'raster_csv',
        'input_files': 'gs://{gcs_bucket}/{gcs_folder}/download/{year}/*.csv',
        'output_dir': 'gs://{gcs_bucket}/{gcs_folder}/{stage}/{year}',
        ...
      },
      {
        'stage': 'events',
        'input_files': 'gs://{gcs_bucket}/{gcs_folder}/raster_csv/{year}/*.csv',
        'output_dir': 'gs://{gcs_bucket}/{gcs_folder}/{stage}/{year}',
        ...
      },
    ],
  }

  See events_pipeline_config.py for list of settings for each stage.
  test_data/sample_fires_event_pipeline_config.py has an example for FireEvent.

Run all stages in the pipeline as:
  python3 events_pipeline.py --pipeline_config=<config.py>

To run a sequence of specific stages, use:
  python3 events_pipeline.py --pipeline_config=<config.py> \
      --run_stages=<stage1>[,<stage2>]
  The config for the stage is assumed to have the input files.
"""

import os
import re
import sys
import time

from absl import app
from absl import flags
from absl import logging

flags.DEFINE_string('pipeline_config', '',
                    'Config for the pipeline as a py dictionary of json')
flags.DEFINE_string(
    'pipeline_state', '',
    'File with state for the pipeline processing as a python dictionary.'
    'Used to resume processing for new incremental data.')
flags.DEFINE_list('run_stages', [],
                  'List of stages to run. If empty, all stages are run.')

_FLAGS = flags.FLAGS

_SCRIPTS_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(_SCRIPTS_DIR)
sys.path.append(os.path.dirname(_SCRIPTS_DIR))
sys.path.append(os.path.dirname(os.path.dirname(_SCRIPTS_DIR)))
sys.path.append(
    os.path.join(os.path.dirname(os.path.dirname(_SCRIPTS_DIR)), 'util'))

import common_flags
import file_util
import pipeline_stage_bigquery
import pipeline_stage_download
import pipeline_stage_earthengine
import pipeline_stage_events
import pipeline_stage_raster_csv
import pipeline_stage_runner
import process_events
import raster_to_csv
import utils

from counters import Counters
from config_map import ConfigMap
from pipeline_stage_runner import StageRunner

# Processing state persisted to a file
# Includes dynamic parameter values for each stage of processing.
_PROCESS_STATE = {
    # Last date for which input was processed in YYYY-MM-DD format
    'last_input_date': '',
}


class EventPipeline(StageRunner):
    '''Class to generate events from source data.
    Runs a series of pipelines as per the config.
    '''

    def __init__(self, config: ConfigMap, counters: Counters = None):
        self._config = config
        if not config:
            self._config = ConfigMap()
        self._pipeline_state = {}
        self.load_pipeline_state()
        config_dicts = [self._config.get('defaults', {})]
        config_dicts.append(self._config.get_configs())
        self.set_up('event_pipeline', config_dicts, self._pipeline_state,
                    counters)
        self.setup_stages()

    def __del__(self):
        # Save the pipeline state into the file.
        if self._pipeline_state:
            pipeline_state_file = self.get_config('pipeline_state_file', '')
            if pipeline_state_file:
                return file_util.file_write_py_dict(self._pipeline_state,
                                                    pipeline_state_file)

    def load_pipeline_state(self) -> dict:
        pipeline_state_file = self._config.get('pipeline_state_file', '')
        if pipeline_state_file:
            self._pipeline_state = file_util.file_load_py_dict(
                pipeline_state_file)
            logging.info(
                f'Loaded pipeline state: {self._pipeline_state} from {pipeline_state_file}'
            )
        return self._pipeline_state

    def set_pipeline_state(self, key: str, value: str):
        '''Sets the value of the pipeline state.'''
        self._pipeline_state[key] = value

    def setup_stages(self):
        '''Create the runners for each stage of the pipeline.'''
        self.stage_runners = []
        stage_names = []
        default_config = self._config.get('defaults', {})
        if not default_config:
            default_config = dict(self._config.get_configs())
            if 'stages' in default_config:
                default_config.remove('stages')
        for stage_config in self._config.get('stages', []):
            stage = stage_config.get('stage', '')
            runner = pipeline_stage_runner.get_stage_runner(stage)
            configs = [default_config, stage_config]
            self.stage_runners.append(
                runner(configs, self._pipeline_state, self.counters))
            stage_names.append(stage)
        logging.info(
            f'Created pipeline with {len(self.stage_runners)} stages: {stage_names}'
        )

    def run_stage(self, stage_name: str, input_files: list = []) -> list:
        '''Run a single stage and return the output files generated.'''
        for stage_runner in self.stage_runners:
            if stage_name == stage_runner.get_name():
                logging.info(f'Running stage {stage_name} with {input_files}')
                return stage_runner.run_stage(input_files)
        logging.error(
            f'No stage runner for {stage_name} with input: {input_files}')
        return []

    def run(self, run_stages: list = []) -> list:
        '''Run all the stages in the pipeline.'''
        stage_count = 0
        output_files = []
        for stage_runner in self.stage_runners:
            stage_count += 1
            stage_name = stage_runner.get_name()
            if not run_stages or stage_name in run_stages:
                self.counters.set_prefix(f'S{stage_count}:{stage_name}:')
                output_files = stage_runner.run_stage(output_files)
            else:
                output_files = []

        if output_files:
            # Update the process state
            self.set_pipeline_state('last_input_date', utils.date_yesterday())
        return output_files


pipeline_stage_runner.register_stage_runner('event_pipeline', EventPipeline)


def main(_):
    config_dict = file_util.file_load_py_dict(_FLAGS.pipeline_config)
    if _FLAGS.pipeline_state:
        config_dict['pipeline_state_file'] = _FLAGS.pipeline_state
    config = ConfigMap(config_dict=config_dict)
    if _FLAGS.debug or config.get('debug', False):
        config.set_config('debug', True)
    pipeline = EventPipeline(config=config)
    pipeline.run(run_stages=_FLAGS.run_stages)


if __name__ == '__main__':
    app.run(main)
