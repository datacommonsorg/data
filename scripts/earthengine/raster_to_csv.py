# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the 'License');
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#         https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an 'AS IS' BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Script to convert geo data from geoTIFF raster or CSV to CSV with
latitude, longitude, s2-cell and data values.

Usage:
# Generate CSV with data fom all bands in a groTiff
# with lat/lng and S2 cell id.
python3 raster_to_csv.py \
      --input_geotiff=<geo-tiff-file-pattern> `# or --input_csv=<file>` \
      --output_csv=<output-data-csv-file> \
      --s2_level=11 `# Convert lat/lng to S2-cell level-11` \
      --output_s2_place=<place-csv-tmcf-prefix> `# output prefix for place files` \
      --output_date=$dt `# Add date columns for data csv`

This produces the following outputs:
1. Data csv file  <output-data-csv-file> with the following columns:
     latitude,longitude,area,date,s2CellId,s2Level,band:0[,band:<N>...]

   Each point in the input is mapped to s2 cells of level s2_level.

2. Place files for new S2 cells Ids generated by this data set.
   2.1 Place csv file <place-csv-tmcf-prefix>.csv for s2 cells with columns:
          s2CellId,typeOf,containedInPlace
      where typeOf is of the form 'dcid:S2CellLevel<N>'
        containeInPlaces is a list of s2 cell ids for all parent levels upto
          --aggregate_s2_level
   2.2 tmcf file <place-csv-tmcf-prefix>.tmcf for the above place file.

Additional processing options:
  --allow_geotiff, --allow_csv
      Only output points that match output in these files.
  --ignore_geotiff, --ignore_csv
      Drop points that match these files.
  --aggregate: Default aggregation for data value columns like 'band:0',
  --config=<json file with config>
    For config options supported, please refer to _DEFAULT_CONFIG below.
"""

import csv
import glob
import math
import multiprocessing
import numpy as np
import os
import rasterio
import re
import s2sphere
import sys
import time

from absl import app
from absl import flags
from absl import logging
from geopy import distance
from typing import Union

_SCRIPTS_DIR = os.path.dirname(__file__)
sys.path.append(_SCRIPTS_DIR)
sys.path.append(os.path.dirname(_SCRIPTS_DIR))
sys.path.append(os.path.dirname(os.path.dirname(_SCRIPTS_DIR)))

import utils

from util.config_map import ConfigMap
from util.counters import Counters, CounterOptions

flags.DEFINE_string('input_geotiff', '', 'GeoTIFF file to process')
flags.DEFINE_string('ignore_geotiff', '',
                    'GeoTIFF with points to be ignored set as data.')
flags.DEFINE_string('allow_geotiff', '',
                    'GeoTIFF with points to be used set as data.')
flags.DEFINE_string('input_csv', '', 'CSV file to process')
flags.DEFINE_string('ignore_csv', '',
                    'CSV with points to be ignored set as data.')
flags.DEFINE_string('allow_csv', '', 'CSV with points to be used set as data.')
flags.DEFINE_integer('limit_points', sys.maxsize,
                     'Number of rows/points to process per input file.')
flags.DEFINE_string('output_csv', '', 'Output CSV file to generate')
flags.DEFINE_string('output_mode', 'w',
                    'Write(w) or append(a) to existing output file.')
flags.DEFINE_string('output_date', None,
                    'Date column to be added to output rows.')
flags.DEFINE_list('output_columns', [], 'Output columns in the CSV')
flags.DEFINE_integer('output_precision', 6,
                     'number of precision digits for float data in putput.')
flags.DEFINE_integer('s2_level', 13, 'S2 Level for S2 cell Id.')
flags.DEFINE_string('output_s2_place', '',
                    'Output prefix for S2 place csv and tmcf.')
flags.DEFINE_list(
    'output_contained_s2_level', [10],
    'Levels to be added as contained in place nodes into s2 place output.')
flags.DEFINE_string('aggregate', 'max',
                    'Aggregate function for data values with same key.')
flags.DEFINE_bool('debug', False, 'Enable debug logs.')
flags.DEFINE_integer('log_every_n', 10000, 'Print logs every N records.')
flags.DEFINE_float('default_cell_area', 0,
                   'Area of the cell if the raster input is not provided.')
flags.DEFINE_string('config', '', 'Config dictionary with parameter settings.')

_FLAGS = flags.FLAGS
_FLAGS(sys.argv)  # Allow invocation without app.run()

_DEFAULT_CONFIG = {
    # Inputs
    'input_geotiff': _FLAGS.input_geotiff,
    'ignore_geotiff': _FLAGS.ignore_geotiff,
    'allow_geotiff': _FLAGS.allow_geotiff,
    'input_csv': _FLAGS.input_csv,
    'ignore_csv': _FLAGS.ignore_csv,
    'allow_csv': _FLAGS.allow_csv,
    'limit_points': _FLAGS.limit_points,

    # Output settings
    'output_csv': _FLAGS.output_csv,
    'output_mode': _FLAGS.output_mode,
    'output_columns': _FLAGS.output_columns,
    'output_date': _FLAGS.output_date,
    'output_precision': _FLAGS.output_precision,
    'output_s2_place': _FLAGS.output_s2_place,
    'output_contained_s2_level': _FLAGS.output_contained_s2_level,

    # Processing parameters
    's2_level': _FLAGS.s2_level,
    # default aggregation for data values mapped to the same s2-cell+date
    # that can be one of: min, max, mean, sum.
    # For columns specific aggregation methods, use config:'input_data_filter'.
    'aggregate': _FLAGS.aggregate,
    # generate data for all s2 cells from --s2_level upto this level.
    # Default cell area if constant for the whole data set.
    'default_cell_area': _FLAGS.default_cell_area,
    # Default point/cell width/height for ~1sqkm at equator, lower near poles.
    'default_cell_width': 0.009,
    'default_cell_height': 0.009,
    # rename columns <input-name>: <output-name>
    'rename_columns': {
        'acq_date': 'date',
        'band:0': 'water',
    },

    # filter settings per column for data
    # input (pre-aggregation) and output (post-aggregation) of the form:
    # {
    #   '<column>' : {
    #     'min': <NN.NNN>, # Minimum value for <column>
    #     'max': <NN.NNN>, # Maximum value for <column>
    #     'aggregate': 'sum', # one of 'min','max','sum','mean'
    #   },
    #   ...
    # }
    'input_data_filter': {
        # Add up area for points added to an s2cell.
        'area': {
            'aggregate': 'sum',
        },
        #'confidence': {
        #    'regex': r'[nh]',  # pick normal or high
        #}
        # 'water': {  # band:0
        #     'min': 1.0
        #},
    },
    'output_data_filter': {
        #   'area': {
        #       'min': 1.0,  # Minimum area in sqkm after aggregation
        #   },
    },

    # Debug settings
    'debug': _FLAGS.debug,  # Enable debug logs
    'log_every_n':
        _FLAGS.log_every_n,  # Show counters when counter increments by N
    'log_every': 'processed_points',  # Counter to check for log_every_n
}

_DEFAULT_COLUMNS = set({'latitude', 'longitude', 's2CellId', 'date'})


def get_raster_data_point(raster: rasterio.io.DatasetReader,
                          arr: np.ndarray,
                          x: int,
                          y: int,
                          lat: float = None,
                          lng: float = None,
                          bands: list = None,
                          nodata_value: int = 0,
                          config: ConfigMap = None,
                          counter: Counters = None) -> dict:
    '''Returns the lat/long and data value for each band
    in the raster for a given position index (x, y).
    Args:
      raster: input raster
      arr: numpy array from the input raster
      x: index of the point
      y: index of the point
      lat: latitude of the point in degrees
      lng: longitude of the point in degrees
        If lat/lng are not set, it is computed from the raster transform
      bands: list of bands indexes to process.
        If not specified, all bands are processed.
      nodata_value: value of cells not having valid data in the raster/numpy
      config: ConfigMap dictionary of config parameters
    Returns:
      dictionary of values for the data point from the raster
      including additional property for s2CellId and area.
    '''
    if not config:
        config = ConfigMap()
    data = dict()
    if lat is None or lng is None:
        (lng, lat) = rasterio.transform.xy(raster.transform, x, y)
    if lat > 90 or lat < -90 or lng > 180 or lng < -180:
        logging.error(f'Invalid lat,lng [{lat},{lng}] for [{x},{y}].')
        counter.add_counter(f'invalid-lat-lng', 1)
        return None
    data['latitude'] = lat
    data['longitude'] = lng
    data['area'] = utils.latlng_cell_area(lat, lng, raster.res[1],
                                          raster.res[0])
    s2_level = config.get('s2_level', None)
    if s2_level:
        data['s2CellId'] = utils.s2_cell_latlng_dcid(lat, lng, s2_level)
        data['s2Level'] = s2_level
    if not bands:
        bands = list(range(raster.count))
    for band in bands:
        point_data = arr[band][x][y]
        if point_data != nodata_value and not np.isnan(point_data):
            data[f'band:{band}'] = point_data
    date = config.get('output_date')
    if date:
        data['date'] = date
    return data


def get_csv_data_point(input_data: dict, config: ConfigMap) -> dict:
    '''Get data for a CSV row.
    Args:
      input_data: dictionary with column:values for a CSV row.
      assumed to have columns: latitude, longitude
    Returns:
      dictionary with values for the data point including area and s2CellId.
    '''
    _set_numeric_data(input_data, config)
    lat = input_data.get('latitude', 0.0)
    lng = input_data.get('longitude', 0.0)
    if isinstance(lat, str) or isinstance(lng, str):
        logging.debug(f'Invalid lat/lng in data {input_data}')
        return {}
    input_data['latitude'] = lat
    input_data['longitude'] = lng
    s2_level = config.get('s2_level', None)
    s2_cell = None
    if s2_level:
        s2_cell = utils.s2_cell_from_latlng(lat, lng, s2_level)
        input_data['s2CellId'] = utils.s2_cell_to_dcid(s2_cell.id())
        input_data['s2Level'] = s2_level
    date = config.get('output_date')
    if 'area' not in input_data:
        area = None
        if s2_cell:
            area = utils.s2_cell_area(s2_cell)
        else:
            default_cell_area = config.get('default_cell_area', 0)
            default_cell_width = config.get('default_cell_width', 0)
            default_cell_height = config.get('default_cell_width', 0)
            if default_cell_area > 0:
                area = default_cell_area
            elif default_cell_width > 0 and default_cell_height > 0:
                area = utils.latlng_cell_area(lat, lng, default_cell_height,
                                              default_cell_width)
        if area:
            input_data['area'] = area
    if date and 'date' not in input_data:
        input_data['date'] = date
    if config.get('ignore_date', False) and 'date' in input_data:
        input_data.pop('date')
    return input_data


def get_raster_data_latlng(raster: rasterio.io.DatasetReader,
                           arr: np.ndarray,
                           lat: float,
                           lng: float,
                           band: int = 0) -> float:
    '''Return the data value for the point (lat, lng) in given raster
    Args:
      raster: raster to look up
      arr: numpy array having values from the ratser
      lat: latitude of the point in degrees
      lng: longitude of the point in degrees
      band: the band index in the numpy array to use.
        uses band 0 if unspecified.
    Returns:
      the data value from the numpy array for the given point.'''
    (x, y) = rasterio.transform.rowcol(raster.transform, lng, lat)
    if not x or not y:
        return None
    (rows, cols) = raster.shape
    if x < 0 or x >= cols or y < 0 or y >= rows:
        logging.debug(
            f'Point {lat}:{lng} ({x}:{y}) outside raster {raster.files}')
        return None
    return arr[band][x][y]


def is_valid_data_point(data_point: dict, filter_params: dict,
                        counter: Counters) -> bool:
    '''Returns True if the data point is allowed for all parameters in config.
    Args:
      data_point: dictionary of values for the point.
         assumes to have properties like latitude/longitude, area, etc
      filter_params:
        dictionary of filter settings for each data value key.
        filter settings could be: min, max
    Returns:
      True if data values satisfy all conditions in the filter params.
    '''
    if filter_params:
        for col, params in filter_params.items():
            value = data_point.get(col, None)
            if 'min' in params:
                if value is None or value < params['min']:
                    counter.add_counter(f'data-dropped-min-{col}', 1)
                    return False
            if 'max' in params:
                if value is None or value > params['max']:
                    counter.add_counter(f'data-dropped-max-{col}', 1)
                    return False
            if 'eq' in params:
                if value is None or not math.isclose(value, params['eq']):
                    counter.add_counter(f'data-dropped-eq-{col}', 1)
                    return False
            if 'regex' in params:
                re_pat = params['regex']
                if value is None or not re.search(re_pat, str(value)):
                    counter.add_counter(f'data-dropped-regex-{col}', 1)
                    return False

    return True


def rename_data_columns(data_point: dict, rename_columns: dict) -> dict:
    '''Rename data point key (columns) names.
    Args:
      data_point: dictionary of data values
      rename_columns: dictionary of key name replacements for data_point dict
    Returns:
      data_point dictionary with the keys renames if any.'''
    if rename_columns:
        for col, new_col in rename_columns.items():
            if col in data_point:
                value = data_point.pop(col)
                data_point[new_col] = value
    return data_point


def get_parent_s2_cellids(lat: float, lng: float, s2cell_id: str, s2_level: int,
                          top_s2_level: int) -> list:
    '''Returns list of parent s2 cell ids(int) up to top_s2_level.'''
    s2cell = None
    if lat is not None and lng is not None:
        s2cell = utils.s2_cell_from_latlng(lat, lng, s2_level)
    elif s2cell_id:
        if isinstance(s2cell_id, str):
            s2cell = utils.s2_cell_from_dcid(s2cell_id)
        else:
            s2cell = s2sphere.CellId(s2cell_id)
    if not s2cell or top_s2_level is None:
        return [0]
    return [
        s2cell.parent(level).id()
        for level in range(s2cell.level(), top_s2_level - 1, -1)
    ]


def add_data_point(data_dict: dict,
                   data_point: dict,
                   config: ConfigMap = {},
                   filter_params: dict = {},
                   counter: Counters = None) -> dict:
    '''Add a data point to the data_dict dictionary
    if a point with the same key already exists, it is updated with new values.
    Args:
      data_dict: dictionary into which the point it to be added.
      data_point: property:values for the data point.
      config: dictionary of config parameters.
        Indicates method to merge existing data, such as:
          sum, min, max, mean.
      filter_params: dictionary with data filter settings.
      counter: dictionary of counters to be updated.
    Returns:
      the data point dictionary added/updated if filter settings are met.
      The returned dict will have additional properties prefixed with '#'
      such as '#count' in case data is merged,
      and band specific counts, like '#band:0:count' when aggregating as mean.
    '''
    counter.add_counter('processed_points', 1)
    rename_data_columns(data_point, config.get('rename_columns', None))
    s2_level = data_point.get('s2Level', config.get('s2Level', ''))
    data_key = _get_data_key(data_point)
    if data_key not in data_dict:
        cur_data = dict(data_point)
    else:
        # Data key already exists. Aggregate.
        cur_data = dict(data_dict[data_key])
        def_aggr = config.get('aggregate', 'sum')
        props = set(data_point.keys()).difference(_DEFAULT_COLUMNS)
        props = [p for p in props if not p.startswith('#')]
        for prop in props:
            cur_val = cur_data.get(prop, 0)
            new_val = data_point[prop]
            aggr = filter_params.get(prop, {}).get('aggregate', def_aggr)
            if aggr == 'sum':
                cur_data[prop] = cur_val + new_val
            elif aggr == 'min':
                cur_data[prop] = min(cur_val, new_val)
            elif aggr == 'max':
                cur_data[prop] = max(cur_val, new_val)
            elif aggr == 'mean':
                cur_num = cur_data.get(f'#{prop}:count', 0)
                new_num = data_point.get(f'#{prop}:count', 1)
                cur_data[prop] = ((cur_val * cur_num) +
                                  (new_val * new_num)) / (cur_num + new_num)
                cur_data[f'#{prop}:count'] = cur_num + new_num
        # Update lat/long to centroid.
        cur_count = cur_data.get('#count', 1)
        new_count = data_point.get('#count', 1)
        for loc in ['latitude', 'longitude']:
            cur_val = cur_data[loc]
            new_val = data_point[loc]
            cur_data[loc] = ((cur_val * cur_count) +
                             (new_val * new_count)) / (cur_count + new_count)
        cur_data['#count'] = cur_count + new_count
        if len(data_dict) % config.get('log_every_n', 1000) == 0:
            logging.debug(f'Added data {data_point} into {cur_data}')
        counter.add_counter(f'processed_points_aggregated_s2level_{s2_level}',
                            1)
    if not is_valid_data_point(cur_data, filter_params, counter):
        counter.add_counter('processed_points_dropped', 1)
        return None
    data_dict[data_key] = cur_data
    counter.add_counter(f'output_points_s2level_{s2_level}', 1)
    return cur_data


def load_raster_geotiff(filename: str) -> rasterio.io.DatasetReader:
    '''Load a raster from a geoTiff file.
    Args:
      filename: Name of the geoTiff file.
    Returns:
      raster data set for the file.
    '''
    logging.info(f'Loading raster GeoTiff File: {filename}')
    src = rasterio.open(filename)
    _log_raster_info(src)
    return src


def write_data_csv(data_points: dict,
                   filename: str,
                   columns: list = None,
                   config: ConfigMap = {},
                   counter: Counters = None):
    '''Output the data points dict into the csv file.
    Args:
      data_points: dictionary of data points keys by location+date.
        each entry is a dictionary of property:values for each point.
        if config doesn't specify output_columns, all keys without the '#' prefix
          are added as csv columns.
      filename: output csv file name
      columns: list of output columns.
          if not set, picks the config['output_columns'] or all keys.
      config: dictionary of parameter:value settings for:
        output_mode: whether to append or overwrite file.
        output_columns: list of columns to emit
          if not specified, all keys in the data_point dict are emitted.

      counter: dictionary of counter:values.
    '''
    # Write to the output.
    if not columns:
        columns = config.get('output_columns', None)
    if not columns:
        all_columns = _get_all_columns(data_points, config,
                                       config.get('ignore_commented', True))
        # Order columns in same order as input.
        columns = []
        for col in config.get('input_columns', []):
            if col in all_columns:
                columns.append(col)
        for col in all_columns:
            if col not in columns:
                columns.append(col)
    logging.info(
        f'Writing {len(data_points)} rows with columns: {columns} into {filename} ...'
    )
    # create output directory if needed
    dirname = os.path.dirname(filename)
    if dirname:
        os.makedirs(dirname, exist_ok=True)
    # Open file in append mode or overwrite mode.
    output_mode = config.get('output_mode', 'w')
    if output_mode == 'a':
        if not os.path.exists(filename):
            # File doesn't exist yet. Open in write mode and add column headers.
            output_mode = 'w'
    with open(filename, mode=output_mode) as csv_file:
        writer = csv.DictWriter(csv_file,
                                fieldnames=columns,
                                escapechar='\\',
                                extrasaction='ignore',
                                quotechar='"',
                                quoting=csv.QUOTE_NONNUMERIC)
        if output_mode == 'w':
            writer.writeheader()
        output_date = config.get('output_date', None)
        precision_digits = config.get('output_precision', 6)
        counter.set_counter('total_points', len(data_points))
        for key, data in data_points.items():
            # Limit decimals for lat/lng
            for col, val in data.items():
                data[col] = utils.str_format_float(val, precision_digits)
            if output_date:
                data['date'] = output_date
            writer.writerow(data)
            counter.set_counter('points_processed', 1)
    counter.set_counter(f'output_rows:{os.path.basename(filename)}',
                        len(data_points))


def write_s2place_csv_tmcf(data_points: dict,
                           output_prefix: str,
                           config: ConfigMap = {},
                           counter: Counters = None):
    '''Generate CSV+tMCF file for S2 places with hierarchy.
    Args:
      data_ponts: dictionary of data points where each point is a dict with 's2Cellid'
      output_prefix: file name prefix for output csv/tmcf files.
      config: dictionary of config parameters
        output_contained_s2_level: topmost s2 level upto which places are added.
      counter: dictionary of named counters
    '''
    s2_places = {}
    contained_levels = config.get('output_contained_s2_level', [10])
    # Collect all S2 cells ids for data points and its parents.
    for data in data_points.values():
        s2cell_dcid = data.get('s2CellId', None)
        if not s2cell_dcid:
            continue
        s2cell = utils.s2_cell_from_dcid(s2cell_dcid)
        if not s2cell:
            continue
        s2_places[s2cell.id()] = {}
    logging.info(
        f'Generating place data for {len(s2_places)} places upto level {contained_levels} into {output_prefix}.csv/tmcf'
    )
    # Add containedInPlace and Types for each s2 cell.
    for cellid in s2_places.keys():
        s2cell = s2sphere.CellId(cellid)
        contained_in = []
        s2level = s2cell.level()
        for level in contained_levels:
            if level and level < s2level:
                contained_in.append(
                    utils.s2_cell_to_dcid(s2cell.parent(level).id()))
        s2_places[cellid] = {
            's2CellId': utils.s2_cell_to_dcid(s2cell.id()),
            'typeOf': f'dcid:S2CellLevel{s2cell.level()}',
            'containedInPlace': ','.join(contained_in),
            'name': f'Level {s2level} S2 Cell {s2cell.id():#018x}',
        }
    output_prefix = os.path.splitext(output_prefix)[0]
    output_dir = os.path.dirname(output_prefix)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    # Generate the place csv
    counter.set_prefix('8:output_places_csv:')
    write_data_csv(s2_places, f'{output_prefix}.csv',
                   ['s2CellId', 'typeOf', 'containedInPlace', 'name'], config,
                   counter)

    # Generate the place tmcf
    logging.info(f'Generating place tmcf {output_prefix}.tmcf')
    place_tmcf = ['Node: E:Place->E0']
    place_tmcf.append('dcid: C:Place->s2CellId')
    place_tmcf.append('typeOf: C:Place->typeOf')
    place_tmcf.append('containedInPlace: C:Place->containedInPlace')
    place_tmcf.append('name: C:Place->name')
    with open(f'{output_prefix}.tmcf', 'w') as tmcf_file:
        tmcf_file.write('\n'.join(place_tmcf))
        tmcf_file.write('\n')


def process_raster(raster_input: str,
                   config: ConfigMap,
                   data_points: dict = None,
                   counter: Counters = {}) -> dict:
    '''Read a raster geoTiff and output points with data values.
    Args:
      raster_input: raster data set to process loaded from a geoTiff file.
      config: dictionary of parameter:value settings.
      data_points: dictionary into which points from the raster are added.
      counter: dictionary of named counters.
    Returns:
      data_points dict with new points from the raster added.'''
    logging.info(
        f'Processing raster {raster_input} with config: {config.get_configs()}')
    counter.add_counter('input_geotiff_files', 1)
    # Load the geoTiff file
    counter.set_prefix('1:load_raster:')
    src = load_raster_geotiff(raster_input)
    # Convert raster into a numpy array.
    # Note: This creates a large array with the shape of the original raster
    # extent with non-valid data points set to 0.
    arr = src.read()
    # Load any allow/ignore raster masks.
    ignore_geotiff = config.get('ignore_geotiff', None)
    allow_geotiff = config.get('allow_geotiff', None)
    ignore_src, ignore_arr = (None, None)
    allow_src, allow_arr = (None, None)
    if ignore_geotiff:
        ignore_src = load_raster_geotiff(ignore_geotiff)
        ignore_arr = ignore_src.read()
    if allow_geotiff:
        allow_src = load_raster_geotiff(allow_geotiff)
        allow_arr = allow_src.read()
    if data_points is None:
        data_points = dict()

    # Extract all valid data points from the raster into a numpy array.
    # Note: This assumes a sparse raster with few valid data points.
    counter.set_prefix('2:extract_raster_points:')
    idx = np.where(arr != 0)
    num_idx = len(idx[0])
    points_xy = [list(), list()]
    if src.count == 1:
        # Get list of xy points with data for the single band.
        points_xy[0] = idx[1]
        points_xy[1] = idx[2]
    else:
        # For raster with multiple bands, get list of points with value data
        # across any band.
        xy_set = set()
        for i in range(num_idx):
            x = idx[1][i]
            y = idx[2][i]
            if (x, y) not in xy_set:
                xy_set.add((x, y))
                points_xy[0].append(x)
                points_xy[1].append(y)
        # Release memory
        idx = None
        xy_set = None
    num_points = len(points_xy[0])
    counter.add_counter('processed_points', num_points)
    logging.info(
        f'Got {num_points} xy points out of {num_idx} values from raster: {raster_input}.'
    )
    # Process all points in the raster.
    process_raster_points(src, arr, ignore_src, ignore_arr, allow_src,
                          allow_arr, points_xy, 0, num_points, data_points,
                          config, None, counter)
    return data_points


def process_raster_points(src_r: rasterio.io.DatasetReader,
                          src_arr: np.ndarray,
                          ignore_r: rasterio.io.DatasetReader,
                          ignore_arr: np.ndarray,
                          allow_r: rasterio.io.DatasetReader,
                          allow_arr: np.ndarray,
                          points_xy: list,
                          points_start_index: int,
                          num_points: int,
                          data_points: dict,
                          config: ConfigMap,
                          output_csv: str = None,
                          counter: Counters = None) -> bool:
    '''Process a set of raster data points.
    Args:
      src_r: Source raster data set to process
      src_arr: numpy array fr source data set with all bands.
      ignore_r: Raster dataset with data points to be ignores.
      ignore_arr: numpy array for data points to be ignored set as 1.
      allow_r: raster dataset for points to be allowed
      allow_arr: numpy array for points to be allowed set as 1.
      points_xy: list of indexes for x and y [[x], [y]] with
        valid data points in source raster src_r
      points_start_index: start index for subset of points to be processed.
      num_points: number of points to process from the start index.
      data_points: dictionary of data point key:value dicts
        into which processed points are  added
      config: ConfigMap dictionary of configuration parameter:values.
      output_csv: output csv file if data_points dict is to be saved in a csv.
      counter: dictionary of named counter:values
    Returns
      True if the raster was processed.
    '''
    logging.info(
        f'Processing points {num_points} of {len(points_xy[0])} from {points_start_index} from {src_r.files} ...'
    )
    if counter is None:
        counter = Counters()
    counter.set_prefix('0:raster:')
    counter.set_counter('geotiff_width', src_r.shape[1])
    counter.set_counter('geotiff_height', src_r.shape[0])
    counter.set_counter('index_data_points', num_points)
    data_filter = config.get('input_data_filter', {})
    counter.set_prefix('3:extract_raster_latlng:')
    counter.set_counter('total_points', num_points)
    # Get Lat/Lng coordinates for all given x,y points
    # in a batch to reduce number of calls into rasterio.
    latlons = rasterio.transform.xy(src_r.transform, points_xy[0], points_xy[1])
    counter.add_counter('processed_points', len(latlons[0]))
    logging.info(f'Got Lat/Lngs for {len(latlons[0])} raster points.')
    # Go over all data points in range.
    counter.set_prefix('4:process_raster_data:')
    num_points = min(num_points, config.get('limit_points', num_points))
    counter.set_counter('total_points', num_points)
    for point_index in range(points_start_index,
                             points_start_index + num_points):
        x = points_xy[0][point_index]
        y = points_xy[1][point_index]
        lng = latlons[0][point_index]
        lat = latlons[1][point_index]
        data = get_raster_data_point(raster=src_r,
                                     arr=src_arr,
                                     x=x,
                                     y=y,
                                     lat=lat,
                                     lng=lng,
                                     config=config,
                                     counter=counter)
        if data:
            # Check if data point is allowed by the ignore/allow masks.
            if ignore_arr is not None:
                ignore_point = get_raster_data_latlng(ignore_r, ignore_arr,
                                                      data['latitude'],
                                                      data['longitude'])
                if ignore_point:
                    logging.debug(
                        f'Ignoring {data} due to ignore mask: {ignore_point}')
                    counter.add_counter('ignored_points', 1)
                    continue
            if allow_arr is not None:
                allow_point = get_raster_data_latlng(allow_r, allow_arr,
                                                     data['latitude'],
                                                     data['longitude'])
                if not allow_point:
                    logging.debug(
                        f'Ignoring {data} due to allow mask: {allow_point}')
                    counter.add_counter('dropped_points', 1)
                    continue
            if add_data_point(data_points, data, config, data_filter, counter):
                counter.add_counter('output_data_points', 1)
        counter.print_counters_periodically()
    counter.print_counters()
    if output_csv:
        counter.set_prefix('5:output_data_csv:')
        write_data_csv(data_points, output_csv, [], config, counter)
    return True


def process_csv_points(input_csv: str,
                       ignore_csv: str = None,
                       allow_csv: str = None,
                       data_points: dict = None,
                       config: ConfigMap = {},
                       output_csv: str = None,
                       counter: Counters = None) -> bool:
    '''Process data points from CSV.
    Args:
      input_csv: Input csv file to load data points.
         Assumed to have columns: latitude, longitude
      ignore_csv: CSV file with points to be ignored.
      allow_csv: CSV file with points to be allowed.
      data_points: Dictionary of data points keyed by location+date.
        into which new points from input_csv are added.
      config: dictionary of configuration parameter:values.
      output_csv: Output csv file if data is to be saved.
      counter: dictionary of named counter:values.
    Returns:
      True if data was processed.
   '''
    if data_points is None:
        data_points = {}
    if counter is None:
        counter = Counters()
    ignore_points = {}
    if ignore_csv:
        filter_config = dict(config)
        filter_config['ignore_date'] = True
        ignore_points = process_csv_points(input_csv=ignore_csv,
                                           config=filter_config)
        logging.info(
            f'Loaded {len(ignore_points)} ignore points from {ignore_csv}')
    allow_points = {}
    if allow_csv:
        filter_config = dict(config)
        filter_config['ignore_date'] = True
        allow_points = process_csv_points(input_csv=allow_csv,
                                          config=filter_config)
        logging.info(
            f'Loaded {len(allow_points)} allow points from {allow_csv}')
    data_filter = config.get('input_data_filter', {})
    input_csv_files = utils.file_get_matching(input_csv)
    logging.info(f'Processing csv files: {input_csv_files}')
    counter.set_prefix('1:process_csv:')
    for filename in input_csv_files:
        counter.add_counter('total_points',
                            utils.file_estimate_num_rows(filename))
    for filename in input_csv_files:
        counter.add_counter('input_csv_files', 1)
        with open(filename) as csvfile:
            logging.info(f'Processing data from file {filename} ...')
            reader = csv.DictReader(csvfile)
            # Save the input columns
            config.set_config('input_columns', reader.fieldnames)
            # Process required number of input rows.
            num_rows = 0
            max_rows = config.get('limit_points', sys.maxsize)
            for row in reader:
                num_rows += 1
                if num_rows > max_rows:
                    break
                logging.debug(f'Processing CSV row {filename}:{num_rows}:{row}')
                data = get_csv_data_point(row, config)
                if not data:
                    counter.add_counter('csv_processed_points_invalid', 1)
                    continue
                data_key = _get_data_key(data, add_date=False)
                if data_key in ignore_points:
                    logging.debug(
                        f'Ignoring point {data} in ignore list: {ignore_points[data_key]}'
                    )
                    counter.add_counter('csv_points_ignored', 1)
                if allow_points and data_key not in allow_points:
                    logging.debug(f'Ignoring point {data} not in allow list')
                    counter.add_counter('csv_points_not_allowed', 1)
                if add_data_point(data_points, data, config, data_filter,
                                  counter):
                    counter.add_counter('csv_points_added', 1)
                counter.print_counters_periodically()
            logging.info(f'Processed {num_rows} points from {filename}')
            counter.set_counter(f'file-points:{os.path.basename(filename)}',
                                num_rows)
            counter.print_counters()
    return data_points


def filter_data_points(data_points: dict,
                       filter_params: dict = None,
                       config: ConfigMap = {},
                       counter: Counters = {}) -> dict:
    '''Remove data points that don't meet required criteria, like min area.
    Args:
      data_points: dictionary of data points each with a dict of key:values.
      filter_params: parameters for keys of the data points.
        see is_valid_data_point() for parameters supported.
      config: dictionary of configuration parameter:values.
      output_csv: Output csv file if data is to be saved.
      counter: dictionary of named counter:values.
    Returns:
      data points dictionary after removing points.
    '''
    if not filter_params:
        return data_points
    logging.info(
        f'Filtering {len(data_points)} with params: {filter_params} ...')
    counter.set_prefix('6:filter_data:')
    counter.set_counter('total_points', len(data_points))
    for k in list(data_points.keys()):
        data = data_points[k]
        if not is_valid_data_point(data, filter_params, counter):
            counter.add_counter('data-dropped-filter', 1)
            logging.debug(f'Dropping data {data}')
            data_points.pop(k)
        counter.set_counter('points_processed', 1)
    return data_points


def process(input_geotiff: str,
            input_csv: str,
            output_csv: str,
            config: ConfigMap,
            counter: Counters = None):
    '''Process raster or CSV inputs to generate CSV output.
    Args:
      input_geotiff: comma separated list of input geotiff file patterns
      input_csv: comma separated list of input csv files.
      output_csv: output csv file to generate
      config: dictionary of config parameter:values.
    '''
    data_points = {}
    if counter is None:
        counter = Counters(options=CounterOptions(
            processed_counter='processed_points', total_counter='total_points'))
    counter.set_counter('start_time', time.perf_counter())
    if input_geotiff:
        logging.info(
            f'Processing raster {input_geotiff} with config: {config.get_configs()}'
        )
        for geotiff_file in utils.file_get_matching(input_geotiff):
            process_raster(geotiff_file, config, data_points, counter)
    if input_csv:
        logging.info(
            f'Processing csv {input_csv} with config: {config.get_configs()}')
        process_csv_points(input_csv=input_csv,
                           ignore_csv=config.get('ignore_csv', None),
                           allow_csv=config.get('allow_csv', None),
                           data_points=data_points,
                           config=config,
                           counter=counter)
        logging.info(f'Loaded {len(data_points)} points from {input_csv}')
    counter.set_counter('end_time', time.perf_counter())

    filter_data_points(data_points, config.get('output_data_filter', None),
                       config, counter)
    counter.set_prefix('7:output_data_csv:')
    write_data_csv(data_points, output_csv, [], config, counter)
    s2_place_file = config.get('output_s2_place', None)
    if s2_place_file:
        write_s2place_csv_tmcf(data_points, s2_place_file, config, counter)
    counter.print_counters()


def _get_data_key(data: dict, add_date: bool = True) -> str:
    '''Return a key for the data values in dictionary
    Uses s2 cell id if it exists, else the lat/lng
    along with the date.'''
    key = data.get('s2CellId', None)
    if not key:
        key = str(data.get('latitude', '')) + str(data.get('longitude', ''))
    if add_date:
        key += data.get('date', '')
    return key


def _get_all_columns(data_points: dict,
                     config: ConfigMap = {},
                     ignore_commented: bool = True) -> list:
    '''Returns a list of all keys across all data points.'''
    cols = set()
    cols.update(config.get('output_columns', []))
    for data in data_points.values():
        cols.update(list(data.keys()))
    if ignore_commented:
        cols = {c for c in cols if not c.startswith('#')}
    return list(sorted(cols))


def _set_numeric_data(data: dict, config: ConfigMap = {}) -> dict:
    '''Returns dictionary of key:values with values converted from string to number.'''
    for k, v in data.items():
        numeric_value = utils.str_get_numeric_value(v)
        if numeric_value:
            data[k] = numeric_value
    return data


def _log_raster_info(raster: rasterio.io.DatasetReader):
    '''Print raster metadata.'''
    logging.info(f'Raster files: {raster.files}')
    logging.info(f'Raster shape: {raster.shape}')
    logging.info(f'Raster resolution: {raster.res}')
    logging.info(f'Raster bounding box: {raster.bounds}')
    logging.info(f'Raster bands: {raster.count}')
    logging.info(f'Raster no-data value: {raster.nodatavals}')
    logging.info(f'Raster CRS: {raster.crs}')
    logging.info(f'Raster profile: {raster.profile}')


def main(_):
    config = ConfigMap(filename=_FLAGS.config, config_dict=_DEFAULT_CONFIG)
    if config.get('debug', False):
        logging.set_verbosity(2)
    process(_FLAGS.input_geotiff, _FLAGS.input_csv, _FLAGS.output_csv, config)


if __name__ == '__main__':
    app.run(main)
