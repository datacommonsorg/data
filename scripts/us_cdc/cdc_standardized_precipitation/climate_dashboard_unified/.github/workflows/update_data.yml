# This is a GitHub Actions workflow to automate the data pipeline for the Unified Climate Dashboard.
# It runs on a schedule, downloads the latest data, processes it, and commits the updated data file
# back to the repository. This keeps the dashboard's data fresh automatically.

name: Automated Data Refresh

on:
  # Allows you to run this workflow manually from the Actions tab on GitHub.
  workflow_dispatch:

  # Runs the workflow automatically every Sunday at midnight UTC.
  schedule:
    - cron: '0 0 * * 0'

jobs:
  update-climate-data:
    runs-on: ubuntu-latest
    steps:
      - name: Check out the repository
        uses: actions/checkout@v4
        with:
          # This is important if your data files are large and managed with Git LFS.
          lfs: true

      - name: Set up Python 3.9
        uses: actions/setup-python@v5
        with:
          python-version: 3.11.5

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r climate_dashboard_unified/requirements.txt

      - name: Make the pipeline script executable
        run: chmod +x climate_dashboard_unified/run_pipeline.sh

      - name: Run the data pipeline
        run: ./climate_dashboard_unified/run_pipeline.sh

      - name: Commit and push the updated data
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add climate_dashboard_unified/climate_indices.parquet
          # The following command checks if there are any staged changes before committing.
          # This prevents empty commits if the data hasn't changed.
          git diff --staged --quiet || git commit -m "Automated data refresh: Update climate_indices.parquet"
          git push
