# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the 'License');
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#         https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an 'AS IS' BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Script to convert geo data from geoTIFF raster or CSV to CSV with
latitude, longitude, s2-cell and data values.

Usage:
# Generate CSV with data fom all bands in a groTiff
# with lat/lng and S2 cell id.
python3 raster_to_csv.py \
      --input_geotiff=<geo-tiff-file-pattern> `# or --input_csv=<file>` \
      --output_csv=<output-data-csv-file> \
      --s2_level=11 `# Convert lat/lng to S2-cell level-11` \
      --aggregate_s2_level=10 `# Generate data for S2 cells for all levels upto 10` \
      --output_upto_s2_level=11  `# Generate places for all s2 cells upto level 11` \
      --output_s2_place=<place-csv-tmcf-prefix> `# output prefix for place files` \
      --output_date=$dt `# Add date columns for data csv` 

This produces the following outputs:
1. Data csv file  <output-data-csv-file> with the following columns:
     latitude,longitude,area,date,s2CellId,s2Level,band:0[,band:<N>...]

   Each point in the input is mapped to s2 cells for all levels
   from --s2_level to --aggregate_s2_level.

2. Place files for new S2 cells Ids generated by this data set.
   2.1 Place csv file <place-csv-tmcf-prefix>.csv for s2 cells
      upto level in --output_upto_s2_level with the following columns:
          s2CellId,typeOf,containedInPlace
      where typeOf is of the form 'dcid:S2CellLevel<N>'
        containeInPlaces is a list of s2 cell ids for all parent levels upto
          --aggregate_s2_level
   2.2 tmcf file <place-csv-tmcf-prefix>.tmcf for the above place file.

Additional processing options:
  --allow_geotiff, --allow_csv
      Only output points that match output in these files.
  --ignore_geotiff, --ignore_csv
      Drop points that match these files.
  --aggregate: Default aggregation for data value columns like 'band:0',
  --config=<json file with config>
    For config options supported, please refer to _DEFAULT_CONFIG below.
"""

import ast
import csv
import glob
import math
import multiprocessing
import numpy as np
import os
import rasterio
import re
import s2sphere
import sys
import time

from absl import app
from absl import flags
from absl import logging
from geopy import distance
from typing import Union

flags.DEFINE_string('input_geotiff', '', 'GeoTIFF file to process')
flags.DEFINE_string('ignore_geotiff', '',
                    'GeoTIFF with points to be ignored set as data.')
flags.DEFINE_string('allow_geotiff', '',
                    'GeoTIFF with points to be used set as data.')
flags.DEFINE_string('input_csv', '', 'CSV file to process')
flags.DEFINE_string('ignore_csv', '',
                    'CSV with points to be ignored set as data.')
flags.DEFINE_string('allow_csv', '', 'CSV with points to be used set as data.')
flags.DEFINE_integer('limit_points', sys.maxsize,
                     'Number of rows/points to process per input file.')
flags.DEFINE_string('output_csv', '', 'Output CSV file to generate')
flags.DEFINE_string('output_mode', 'w',
                    'Write(w) or append(a) to existing output file.')
flags.DEFINE_string('output_date', None,
                    'Date column to be added to output rows.')
flags.DEFINE_list('output_columns', [], 'Output columns in the CSV')
flags.DEFINE_integer('output_precision', 6,
                     'number of precision digits for float data in putput.')
flags.DEFINE_integer('s2_level', 13, 'S2 Level for S2 cell Id.')
flags.DEFINE_integer('aggregate_s2_level', None,
                     'Aggregate data upwards to s2 level.')
flags.DEFINE_string('output_s2_place', '',
                    'Output prefix for S2 place csv and tmcf.')
flags.DEFINE_integer('output_upto_s2_level', 10,
                     'Top S2 level place node in --output_s2_place csv.')
flags.DEFINE_integer(
    'output_contained_s2_level', 10,
    'Levels to be added as contained in place nodes into s2 place output.')
flags.DEFINE_string('aggregate', 'max',
                    'Aggregate function for data values with same key.')
flags.DEFINE_bool('debug', False, 'Enable debug logs.')
flags.DEFINE_integer('log_every_n', 10000, 'Print logs every N records.')
flags.DEFINE_integer('parallelism', 1,
                     'Number of parallel threads for data processing.')
flags.DEFINE_float('default_cell_area', 0,
                   'Area of the cell if the raster input is not provided.')
flags.DEFINE_string('config', '', 'Config dictionary with parameter settings.')

_FLAGS = flags.FLAGS
_FLAGS(sys.argv)  # Allow invocation without app.run()

_DEFAULT_CONFIG = {
    # Inputs
    'input_geotiff': _FLAGS.input_geotiff,
    'ignore_geotiff': _FLAGS.ignore_geotiff,
    'allow_geotiff': _FLAGS.allow_geotiff,
    'input_csv': _FLAGS.input_csv,
    'ignore_csv': _FLAGS.ignore_csv,
    'allow_csv': _FLAGS.allow_csv,
    'limit_points': _FLAGS.limit_points,

    # Output settings
    'output_csv': _FLAGS.output_csv,
    'output_mode': _FLAGS.output_mode,
    'output_columns': _FLAGS.output_columns,
    'output_date': _FLAGS.output_date,
    'output_precision': _FLAGS.output_precision,
    'output_s2_place': _FLAGS.output_s2_place,
    'output_upto_s2_level': _FLAGS.output_upto_s2_level,
    'output_contained_s2_level': _FLAGS.output_contained_s2_level,

    # Processing parameters
    's2_level': _FLAGS.s2_level,
    # default aggregation for data values mapped to the same s2-cell+date
    # that can be one of: min, max, mean, sum.
    # For columns specific aggregation methods, use config:'input_data_filter'.
    'aggregate': _FLAGS.aggregate,
    # generate data for all s2 cells from --s2_level upto this level.
    'aggregate_s2_level': _FLAGS.aggregate_s2_level,
    # Default cell area if constant for the whole data set.
    'default_cell_area': _FLAGS.default_cell_area,
    # Default point/cell width/height for ~1sqkm at equator, lower near poles.
    'default_cell_width': 0.009,
    'default_cell_height': 0.009,
    # rename columns <input-name>: <output-name>
    'rename_columns': {
        'acq_date': 'date',
        'band:0': 'water',
    },
    # Unused
    'parallelism': _FLAGS.parallelism,

    # filter settings per column for data
    # input (pre-aggregation) and output (post-aggregation) of the form:
    # {
    #   '<column>' : {
    #     'min': <NN.NNN>, # Minimum value for <column>
    #     'max': <NN.NNN>, # Maximum value for <column>
    #     'aggregate': 'sum', # one of 'min','max','sum','mean'
    #   },
    #   ...
    # }
    'input_data_filter': {
        # Add up area for points added to an s2cell.
        'area': {
            'aggregate': 'sum',
        },
        'confidence': {
            'regex': r'[nh]',  # pick normal or high
        }
        # 'water': {  # band:0
        #     'min': 1.0
        #},
    },
    'output_data_filter': {
        #   'area': {
        #       'min': 1.0,  # Minimum area in sqkm after aggregation
        #   },
    },

    # Debug settings
    'debug': _FLAGS.debug,  # Enable debug logs
    'log_every_n':
        _FLAGS.log_every_n,  # Show counters when counter increments by N
    'log_every': 'input_points',  # Counter to check for log_every_n
}

_DEFAULT_COLUMNS = set({'latitude', 'longitude', 's2CellId', 'date'})

_EARTH_RADIUS = 6371  # Radius of Earth in Km.


def load_config(config: str, default_config: dict = _DEFAULT_CONFIG) -> dict:
    '''Load config from string or file.'''
    config_dict = dict(default_config)
    config_str = config
    if isinstance(config, str):
        if os.path.exists(config):
            with open(config, 'r') as config_file:
                config_str = config_file.read()
        if config_str:
            config_dict.update(ast.literal_eval(config_str))
    elif isinstance(config, dict):
        config_dict.update(config)
    logging.info(f'Using config: {config_dict}')
    return config_dict


def _add_counter(counter: dict, name: str, value: int):
    '''Increment a counter.'''
    if counter and name:
        counter[name] = counter.get(name, 0) + value


def _set_counter(counter: dict, name: str, value: int):
    '''Set a counter value.'''
    counter[name] = value


def _set_rate_counter(counter: dict, config: dict = {}):
    '''Add a rate counter.'''
    start_time = counter.get('start_time', None)
    if start_time:
        num_items = counter.get('file_points_processed', 0)
        end_time = counter.get('end_time', time.perf_counter())
        time_taken = end_time - start_time
        rate = counter.get('input_points', 0) / (time_taken + .0001) + 0.0001
        _set_counter(counter, 'processing_time_sec', time_taken)
        _set_counter(counter, 'processing_rate', rate)
        total_items = counter.get('file_input_points', 0)
        if total_items:
            remaining_items = max(0, total_items - num_items)
            _set_counter(counter, 'file_remaining_time_sec',
                         remaining_items / rate)


def _show_counters(counter: dict, config: dict = None):
    '''Show all numeric counters periodically if config is given.'''
    if config:
        every_n = config.get('log_every_n', 10000)
        name = config.get('log_every', 'input_points')
        if name not in counter or counter[name] % every_n > 0:
            return
        _set_rate_counter(counter, config)
    counter_str = '\n'.join([
        f'{k:>40s} = {v:10.2f}' for k, v in counter.items()
        if not isinstance(v, str)
    ])
    logging.info(f'Counters:\n{counter_str}')


def _format_val(data: float, precision_digits: int = 6):
    if isinstance(data, float):
        return f'{data:.{precision_digits}f}'
    return data


def _s2cell(lat: float, lng: float, level: int) -> s2sphere.CellId:
    assert level >= 0 and level <= 30
    ll = s2sphere.LatLng.from_degrees(lat, lng)
    cell = s2sphere.CellId.from_lat_lng(ll)
    if level < 30:
        cell = cell.parent(level)
    return cell


def _s2cell_dcid(s2cell_id: int) -> str:
    return f'dcid:s2CellId/{s2cell_id:#018x}'


def _s2cell_from_dcid(s2_dcid: str) -> s2sphere.CellId:
    return s2sphere.CellId(int(s2_dcid[s2_dcid.find('/') + 1:], 16))


def _latlng_s2cell_dcid(lat: float, lng: float, level: int) -> str:
    return _s2cell_dcid(_s2cell(lat, lng, level).id())


def _get_files(filepat: str) -> list:
    '''Return list of matching file names.'''
    files = list()
    for file in filepat.split(','):
        for f in glob.glob(file):
            if f not in files:
                files.append(f)
    return files


def _get_data_key(data: dict, add_date: bool = True) -> str:
    '''Return a key for the data values in dictionary
    Uses s2 cell id if it exists, else the lat/lng
    along with the date.'''
    key = data.get('s2CellId', None)
    if not key:
        key = data.get('latitude', '') + data.get('longitude', '')
    if add_date:
        key += data.get('date', '')
    return key


def _get_all_columns(data_points: dict,
                     config: dict = {},
                     ignore_commented: bool = True) -> list:
    '''Returns a list of all keys across all data points.'''
    cols = set()
    cols.update(config.get('output_columns', []))
    for data in data_points.values():
        cols.update(list(data.keys()))
    if ignore_commented:
        cols = {c for c in cols if not c.startswith('#')}
    return list(sorted(cols))


def _get_numeric_value(value: str) -> Union[int, float, None]:
    '''Returns the float value from string or None.'''
    if isinstance(value, int) or isinstance(value, float):
        return value
    if value and isinstance(value, str):
        try:
            normalized_value = value
            if (value[0].isdigit() or value[0] == '.' or value[0] == '-' or
                    value[0] == '+'):
                # Input looks like a number. Remove allowed extra characters.
                normalized_value = normalized_value.replace(',', '')
                if value.count('.') > 1:
                    # Period may be used instead of commas. Remove it.
                    normalized_value = normalized_value.replace('.', '')
            if value.count('.') == 1:
                return float(normalized_value)
            return int(normalized_value)
        except ValueError:
            # Value is not a number. Ignore it.
            return None
    return None


def _set_numeric_data(data: dict, config: dict = {}) -> dict:
    '''Returns dictionary of key:values with values converted from string to number.'''
    for k, v in data.items():
        numeric_value = _get_numeric_value(v)
        if numeric_value:
            data[k] = numeric_value
    return data


def show_raster_info(raster: rasterio.io.DatasetReader):
    '''Print metadata details about the raster.'''
    logging.info(f'Raster files: {raster.files}')
    logging.info(f'Raster shape: {raster.shape}')
    logging.info(f'Raster resolution: {raster.res}')
    logging.info(f'Raster bounding box: {raster.bounds}')
    logging.info(f'Raster bands: {raster.count}')
    logging.info(f'Raster no-data value: {raster.nodatavals}')
    logging.info(f'Raster CRS: {raster.crs}')
    logging.info(f'Raster profile: {raster.profile}')


def get_cell_area(lat: float, lng: float, height: float, width: float) -> float:
    '''Returns the area of the rectangular cell in km2.
    Args:
      lat: latitude of a corner.
      lng: Longitude of a corner
      width: width in degrees
      height: height in degrees
    Returns:
      area in square kilometers.'''
    try:
        bottom_left = (lat, lng)
        top_left = (min(90, lat + height), lng)
        bottom_right = (lat, min(lng + width, 180))
        width_km = distance.geodesic(bottom_left, bottom_right).km
        height_km = distance.geodesic(top_left, bottom_left).km
        return width_km * height_km
    except ValueError:
        logging.error(f'Invalid coordinates for area: {locals()}')
        return 0


def get_s2_cell_area(cell_id: s2sphere.CellId) -> float:
    '''Returns the area of the S2 cel lin sqkm

     Converts the are of the S2 cell into sqkm using a fixed radius of 6371 km.

     Args:
       cell_id: S2 CellId

     Returns:
       Area of the cell in sq km.
     '''
    return s2sphere.Cell(cell_id).exact_area() * _EARTH_RADIUS * _EARTH_RADIUS


def get_raster_data_point(raster: rasterio.io.DatasetReader,
                          arr: np.ndarray,
                          x: int,
                          y: int,
                          lat: float = None,
                          lng: float = None,
                          bands: list = None,
                          nodata_value: int = 0,
                          config: dict = {},
                          counter: dict = None) -> dict:
    '''Returns the lat/long and data value for each band
    in the raster for a given position index (x, y).
    Args:
      raster: input raster
      arr: numpy array from the input raster
      x: index of the point
      y: index of the point
      lat: latitude of the point in degrees
      lng: longitude of the point in degrees
        If lat/lng are not set, it is computed from the raster transform
      bands: list of bands indexes to process.
        If not specified, all bands are processed.
      nodata_value: value of cells not having valid data in the raster/numpy
      config: dictionary of config parameters
    Returns:
      dictionary of values for the data point from the raster
      including additional property for s2CellId and area.
    '''
    data = dict()
    if lat is None or lng is None:
        (lng, lat) = rasterio.transform.xy(raster.transform, x, y)
    if lat > 90 or lat < -90 or lng > 180 or lng < -180:
        logging.error(f'Invalid lat,lng [{lat},{lng}] for [{x},{y}].')
        _add_counter(counter, f'invalid-lat-lng', 1)
        return None
    data['latitude'] = lat
    data['longitude'] = lng
    data['area'] = get_cell_area(lat, lng, raster.res[1], raster.res[0])
    s2_level = config.get('s2_level', None)
    if s2_level:
        data['s2CellId'] = _latlng_s2cell_dcid(lat, lng, s2_level)
        data['s2Level'] = s2_level
    if not bands:
        bands = list(range(raster.count))
    for band in bands:
        point_data = arr[band][x][y]
        if point_data != nodata_value:
            data[f'band:{band}'] = point_data
    date = config.get('output_date')
    if date:
        data['date'] = date
    return data


def get_csv_data_point(input_data: dict, config: dict) -> dict:
    '''Get data for a CSV row.
    Args:
      input_data: dictionary with column:values for a CSV row.
      assumed to have columns: latitude, longitude
    Returns:
      dictionary with values for the data point including area and s2CellId.
    '''
    _set_numeric_data(input_data, config)
    lat = input_data.get('latitude', 0.0)
    lng = input_data.get('longitude', 0.0)
    if isinstance(lat, str) or isinstance(lng, str):
        logging.debug(f'Invalid lat/lng in data {input_data}')
        return {}
    input_data['latitude'] = lat
    input_data['longitude'] = lng
    s2_level = config.get('s2_level', None)
    s2_cell = None
    if s2_level:
        s2_cell = _s2cell(lat, lng, s2_level)
        input_data['s2CellId'] = _s2cell_dcid(s2_cell.id())
        input_data['s2Level'] = s2_level
    date = config.get('output_date')
    if 'area' not in input_data:
        area = None
        if s2_cell:
            area = get_s2_cell_area(s2_cell)
        else:
            default_cell_area = config.get('default_cell_area', 0)
            default_cell_width = config.get('default_cell_width', 0)
            default_cell_height = config.get('default_cell_width', 0)
            if default_cell_area > 0:
                area = default_cell_area
            elif default_cell_width > 0 and default_cell_height > 0:
                area = get_cell_area(lat, lng, default_cell_height,
                                     default_cell_width)
        if area:
            input_data['area'] = area
    if date and 'date' not in input_data:
        input_data['date'] = date
    if config.get('ignore_date', False) and 'date' in input_data:
        input_data.pop('date')
    return input_data


def get_raster_data_latlng(raster: rasterio.io.DatasetReader,
                           arr: np.ndarray,
                           lat: float,
                           lng: float,
                           band: int = 0) -> float:
    '''Return the data value for the point (lat, lng) in given raster
    Args:
      raster: raster to look up
      arr: numpy array having values from the ratser
      lat: latitude of the point in degrees
      lng: longitude of the point in degrees
      band: the band index in the numpy array to use.
        uses band 0 if unspecified.
    Returns:
      the data value from the numpy array for the given point.'''
    (x, y) = rasterio.transform.rowcol(raster.transform, lng, lat)
    if not x or not y:
        return None
    (rows, cols) = raster.shape
    if x < 0 or x >= cols or y < 0 or y >= rows:
        logging.debug(
            f'Point {lat}:{lng} ({x}:{y}) outside raster {raster.files}')
        return None
    return arr[band][x][y]


def is_valid_data_point(data_point: dict, filter_params: dict,
                        counter: dict) -> bool:
    '''Returns True if the data point is allowed for all parameters in config.
    Args:
      data_point: dictionary of values for the point.
         assumes to have properties like latitude/longitude, area, etc
      filter_params:
        dictionary of filter settings for each data value key.
        filter settings could be: min, max
    Returns:
      True if data values satisfy all conditions in the filter params.
    '''
    if filter_params:
        for col, params in filter_params.items():
            if col in data_point:
                value = data_point[col]
                if 'min' in params:
                    if value < params['min']:
                        _add_counter(counter, f'data-dropped-min-{col}', 1)
                        return False
                if 'max' in params:
                    if value > params['max']:
                        _add_counter(counter, f'data-dropped-max-{col}', 1)
                        return False
                if 'eq' in params:
                    if not math.isclose(value, params['eq']):
                        _add_counter(counter, f'data-dropped-eq-{col}', 1)
                        return False
                if 'regex' in params:
                    re_pat = params['regex']
                    if not re.search(re_pat, str(value)):
                      _add_counter(counter, f'data-dropped-regex-{col}', 1)
                      return False

    return True


def rename_data_columns(data_point: dict, rename_columns: dict) -> dict:
    '''Rename data point key (columns) names.
    Args:
      data_point: dictionary of data values
      rename_columns: dictionary of key name replacements for data_point dict
    Returns:
      data_point dictionary with the keys renames if any.'''
    if rename_columns:
        for col, new_col in rename_columns.items():
            if col in data_point:
                value = data_point.pop(col)
                data_point[new_col] = value
    return data_point


def get_parent_s2_cellids(lat: float, lng: float, s2cell_id: str, s2_level: int,
                          top_s2_level: int) -> list:
    '''Returns list of parent s2 cell ids(int) up to top_s2_level.'''
    s2cell = None
    if lat is not None and lng is not None:
        s2cell = _s2cell(lat, lng, s2_level)
    elif s2cell_id:
        if isinstance(s2cell_id, str):
            s2cell = _s2cell_from_dcid(s2cell_id)
        else:
            s2cell = s2sphere.CellId(s2cell_id)
    if not s2cell or top_s2_level is None:
        return [0]
    return [
        s2cell.parent(level).id()
        for level in range(s2cell.level(), top_s2_level - 1, -1)
    ]


def add_data_point(data_dict: dict,
                   data_point: dict,
                   config: dict = {},
                   filter_params: dict = None,
                   counter: dict = None) -> dict:
    '''Add a data point to the data_dict dictionary
    if a point with the same key already exists, it is updated with new values.
    Args:
      data_dict: dictionary into which the point it to be added.
      data_point: property:values for the data point.
      config: dictionary of config parameters.
        Indicates method to merge existing data, such as:
          sum, min, max, mean.
      filter_params: dictionary with data filter settings.
      counter: dictionary of counters to be updated.
    Returns:
      the data point dictionary added/updated if filter settings are met.
      The returned dict will have additional properties prefixed with '#'
      such as '#count' in case data is merged,
      and band specific counts, like '#band:0:count' when aggregating as mean.
    '''
    _add_counter(counter, 'input_points', 1)
    _add_counter(counter, 'file_points_processed', 1)
    rename_data_columns(data_point, config.get('rename_columns', None))
    s2_level = config.get('s2_level', None)
    s2cells = get_parent_s2_cellids(lat=data_point.get('latitude', None),
                                    lng=data_point.get('longitude', None),
                                    s2cell_id=data_point.get('s2CellId', None),
                                    s2_level=s2_level,
                                    top_s2_level=config.get(
                                        'aggregate_s2_level', None))
    if s2_level is None:
        s2_level = 30
    for cell_id in s2cells:
        if cell_id:
            data_point['s2CellId'] = _s2cell_dcid(cell_id)
            data_point['s2Level'] = s2sphere.CellId(cell_id).level()
        data_key = _get_data_key(data_point)
        if data_key not in data_dict:
            cur_data = dict(data_point)
            _add_counter(counter, f'output_points_s2level_{s2_level}', 1)
        else:
            # Data key already exists. Aggregate.
            cur_data = dict(data_dict[data_key])
            def_aggr = config.get('aggregate', 'sum')
            props = set(data_point.keys()).difference(_DEFAULT_COLUMNS)
            props = [p for p in props if not p.startswith('#')]
            for prop in props:
                cur_val = cur_data.get(prop, 0)
                new_val = data_point[prop]
                aggr = filter_params.get(prop, {}).get('aggregate', def_aggr)
                if aggr == 'sum':
                    cur_data[prop] = cur_val + new_val
                elif aggr == 'min':
                    cur_data[prop] = min(cur_val, new_val)
                elif aggr == 'max':
                    cur_data[prop] = max(cur_val, new_val)
                elif aggr == 'mean':
                    cur_num = cur_data.get(f'#{prop}:count', 0)
                    new_num = data_point.get(f'#{prop}:count', 1)
                    cur_data[prop] = ((cur_val * cur_num) +
                                      (new_val * new_num)) / (cur_num + new_num)
                    cur_data[f'#{prop}:count'] = cur_num + new_num
            # Update lat/long to centroid.
            cur_count = cur_data.get('#count', 1)
            new_count = data_point.get('#count', 1)
            for loc in ['latitude', 'longitude']:
                cur_val = cur_data[loc]
                new_val = data_point[loc]
                cur_data[loc] = (
                    (cur_val * cur_count) +
                    (new_val * new_count)) / (cur_count + new_count)
            cur_data['#count'] = cur_count + new_count
            if len(data_dict) % config.get('log_every_n', 1000) == 0:
                logging.debug(f'Added data {data_point} into {cur_data}')
            _add_counter(counter, f'input_points_aggregated_s2level_{s2_level}',
                         1)
        if not is_valid_data_point(cur_data, filter_params, counter):
            _add_counter(counter, 'input_points_dropped', 1)
            continue
        data_dict[data_key] = cur_data
        s2_level -= 1
    return cur_data


def load_raster_geotiff(filename: str) -> rasterio.io.DatasetReader:
    '''Load a raster from a geoTiff file.
    Args:
      filename: Name of the geoTiff file.
    Returns:
      raster data set for the file.
    '''
    logging.info(f'Loading raster GeoTiff File: {filename}')
    src = rasterio.open(filename)
    show_raster_info(src)
    return src


def write_data_csv(data_points: dict,
                   filename: str,
                   columns: list = None,
                   config: dict = {},
                   counter: dict = None):
    '''Output the data points dict into the csv file.
    Args:
      data_points: dictionary of data points keys by location+date.
        each entry is a dictionary of property:values for each point.
        if config doesn't specify output_columns, all keys without the '#' prefix
          are added as csv columns.
      filename: output csv file name
      columns: list of output columns.
          if not set, picks the config['output_columns'] or all keys.
      config: dictionary of parameter:value settings for:
        output_mode: whether to append or overwrite file.
        output_columns: list of columns to emit
          if not specified, all keys in the data_point dict are emitted.

      counter: dictionary of counter:values.
    '''
    # Write to the output.
    if not columns:
        columns = config.get('output_columns', None)
    if not columns:
        all_columns = _get_all_columns(data_points, config,
                                   config.get('ignore_commented', True))
        # Order columns in same order as input.
        columns = []
        for col in config.get('input_columns', []):
          if col in all_columns:
            columns.append(col)
        for col in all_columns:
          if col not in columns:
            columns.append(col)
    logging.info(
        f'Writing {len(data_points)} rows with columns: {columns} into {filename} ...'
    )
    # create output directory if needed
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    # Open file in append mode or overwrite mode.
    output_mode = config.get('output_mode', 'w')
    if output_mode == 'a':
        if not os.path.exists(filename):
            # File doesn't exist yet. Open in write mode and add column headers.
            output_mode = 'w'
    with open(filename, mode=output_mode) as csv_file:
        writer = csv.DictWriter(csv_file,
                                fieldnames=columns,
                                escapechar='\\',
                                extrasaction='ignore',
                                quotechar='"',
                                quoting=csv.QUOTE_NONNUMERIC)
        if output_mode == 'w':
            writer.writeheader()
        output_date = config.get('output_date', None)
        precision_digits = config.get('output_precision', 6)
        for key, data in data_points.items():
            # Limit decimals for lat/lng
            for col, val in data.items():
                data[col] = _format_val(val, precision_digits)
            if output_date:
                data['date'] = output_date
            writer.writerow(data)
    _set_counter(counter, f'output_rows:{os.path.basename(filename)}',
                 len(data_points))


def write_s2place_csv_tmcf(data_points: dict,
                           output_prefix: str,
                           config: dict = {},
                           counter: dict = None):
    '''Generate CSV+tMCF file for S2 places with hierarchy.
    Args:
      data_ponts: dictionary of data points where each point is a dict with 's2Cellid'
      output_prefix: file name prefix for output csv/tmcf files.
      config: dictionary of config parameters
        output_upto_s2_level: topmost s2 level upto which places are added.
        aggregate_s2_level: topmost s2 cell level upto which data points are generated
      counter: dictionary of named counters
    '''
    s2_places = {}
    top_contained_s2_level = config.get('aggregate_s2_level', 10)
    top_output_s2_level = config.get('output_upto_s2_level',
                                     config.get('s2_level', 10))
    contained_levels = [
        config.get('output_contained_s2_level', top_contained_s2_level)
    ]
    # Collect all S2 cells ids for data points and its parents.
    for data in data_points.values():
        s2cell_dcid = data.get('s2CellId', None)
        if not s2cell_dcid:
            continue
        s2cell = _s2cell_from_dcid(s2cell_dcid)
        if not s2cell:
            continue
        if s2cell.level() < top_output_s2_level:
            continue
        s2_places[s2cell.id()] = {}
        #for level in contained_levels:
        #    s2_places[s2cell.parent(level).id()] = {}
    logging.info(
        f'Generating place data for {len(s2_places)} places upto level {top_output_s2_level} into {output_prefix}.csv/tmcf'
    )
    # Add containedInPlace and Types for each s2 cell.
    for cellid in s2_places.keys():
        s2cell = s2sphere.CellId(cellid)
        contained_in = []
        s2level = s2cell.level()
        for level in contained_levels:
            if level:
                contained_in.append(_s2cell_dcid(s2cell.parent(level).id()))
        s2_places[cellid] = {
            's2CellId': _s2cell_dcid(s2cell.id()),
            'typeOf': f'dcid:S2CellLevel{s2cell.level()}',
            'containedInPlace': ','.join(contained_in),
            'name': f'Level {s2level} S2 Cell {s2cell.id():#018x}',
        }
    output_prefix = os.path.splitext(output_prefix)[0]
    os.makedirs(os.path.dirname(output_prefix), exist_ok=True)
    # Generate the place csv
    write_data_csv(s2_places, f'{output_prefix}.csv',
                   ['s2CellId', 'typeOf', 'containedInPlace', 'name'], config,
                   counter)

    # Generate the place tmcf
    logging.info(f'Generating place tmcf {output_prefix}.tmcf')
    place_tmcf = ['Node: E:Place->E0']
    place_tmcf.append('dcid: C:Place->s2CellId')
    place_tmcf.append('typeOf: C:Place->typeOf')
    place_tmcf.append('containedInPlace: C:Place->containedInPlace')
    place_tmcf.append('name: C:Place->name')
    with open(f'{output_prefix}.tmcf', 'w') as tmcf_file:
        tmcf_file.write('\n'.join(place_tmcf))
        tmcf_file.write('\n')


def process_raster(raster_input: str,
                   config: dict = {},
                   data_points: dict = None,
                   counter: dict = {}) -> dict:
    '''Read a raster geoTiff and output points with data values.
    Args:
      raster_input: raster data set to process loaded from a geoTiff file.
      config: dictionary of parameter:value settings.
      data_points: dictionary into which points from the raster are added.
      counter: dictionary of named counters.
    Returns:
      data_points dict with new points from the raster added.'''
    logging.info(f'Processing raster {raster_input} with config: {config}')
    _add_counter(counter, 'input_geotiff_files', 1)
    # Load the geoTiff file
    start_time = time.perf_counter()
    src = load_raster_geotiff(raster_input)
    # Convert raster into a numpy array.
    # Note: This creates a large array with the shape of the original raster
    # extent with non-valid data points set to 0.
    arr = src.read()
    # Load any allow/ignore raster masks.
    ignore_geotiff = config.get('ignore_geotiff', None)
    allow_geotiff = config.get('allow_geotiff', None)
    ignore_src, ignore_arr = (None, None)
    allow_src, allow_arr = (None, None)
    if ignore_geotiff:
        ignore_src = load_raster_geotiff(ignore_geotiff)
        ignore_arr = ignore_src.read()
    if allow_geotiff:
        allow_src = load_raster_geotiff(allow_geotiff)
        allow_arr = allow_src.read()
    _set_counter(counter, 'geotiff_load_time', time.perf_counter() - start_time)
    if data_points is None:
        data_points = dict()

    # Extract all valid data points from the raster into a numpy array.
    # Note: This assumes a sparse raster with few valid data points.
    idx = np.where(arr != 0)
    num_points = len(idx[0])
    logging.info(f'Got {num_points} points from raster: {raster_input}.')
    parallelism = config.get('parallelism', 1)
    if parallelism <= 1:
        # Process all points in the raster.
        process_raster_points(src, arr, ignore_src, ignore_arr, allow_src,
                              allow_arr, idx, 0, num_points, data_points,
                              config, None, counter)
    else:
        # Batch points to be processed and
        # spawn separate process for each batch of points.
        # TODO: rasterio blocks spawn, release the GPI lock to allow.
        with multiprocessing.Pool(parallelism) as pool:
            batch_size = int(num_points / parallelism)
            logging.info(
                f'Launching {parallelism} processes each with {batch_size} of {num_points} points ...'
            )
            batch_args = []
            if not output_csv:
                output_csv = 'raster_csv'
            output_prefix, ext = os.path.splitext(output_csv)
            results = []
            for batch in range(parallelism):
                # Spawn a process for each batch of points.
                # output from each batch is written into a csv file.
                output_file = f'{output_prefix}-{batch:05d}-of-{parallelism:05d}.csv'
                batch_config = dict(config)
                batch_config['parallelism'] = 1
                batch_config['ignore_commented'] = False
                process_args = {
                    'src_r': src,
                    'src_arr': arr,
                    'ignore_r': ignore_src,
                    'ignore_arr': ignore_arr,
                    'allow_r': allow_src,
                    'allow_arr': allow_arr,
                    'points_index': idx,
                    'points_start_index': batch * batch_size,
                    'num_points': batch_size,
                    'data_points': None,
                    'output_csv': output_file,
                    'config': batch_config,
                    'counter': {},
                }
                batch_args.append(process_args)
                logging.info(
                    f'Launching process:{batch} with args:{process_args}')
                async_result = pool.apply_async(process_raster_points,
                                                kwds=process_args)
                logging.info(f'AsyncRes: {async_result}')
                results.append(async_result)
            # Wait for all child processes to complete.
            logging.info(f'Waiting for {parallelism} processes to complete ...')
            pool.close()
            pool.join()
            for batch in range(len(results)):
                logging.info(f'Waiting for process {batch} to complete ...')
                results[batch].wait()
                result = results[batch].get()
                logging.info(f'Process {batch} result: {result}')

        # Merge the data points from all batched process csv outputs.
        logging.info(f'Merging data from {parallelism} shards ...')
        for args in batch_args:
            # Load the data points from the output file.
            filename = args['output_csv']
            logging.info(f'Waiting for data points file {filename} ...')
            # Wait until file is ready.
            for timeout in range(config.get('timeout', 10)):
                if os.path.exists(filename):
                    break
                time.sleep(1)
            logging.info(f'Merging data points from {filename} ...')
            process_csv_points(input_csv=filename,
                               data_points=data_points,
                               config=config)
            _set_counter(counter, 'output_data_points', len(data_points))
            _show_counters(counter)
    return data_points


def process_raster_points(src_r: rasterio.io.DatasetReader,
                          src_arr: np.ndarray,
                          ignore_r: rasterio.io.DatasetReader,
                          ignore_arr: np.ndarray,
                          allow_r: rasterio.io.DatasetReader,
                          allow_arr: np.ndarray,
                          points_index: list,
                          points_start_index: int,
                          num_points: int,
                          data_points: dict,
                          config: dict = {},
                          output_csv: str = None,
                          counter: dict = None) -> bool:
    '''Process a set of raster data points.
    Args:
      src_r: Source raster data set to process
      src_arr: numpy array fr source data set with all bands.
      ignore_r: Raster dataset with data points to be ignores.
      ignore_arr: numpy array for data points to be ignored set as 1.
      allow_r: raster dataset for points to be allowed
      allow_arr: numpy array for points to be allowed set as 1.
      points_index: tuple of ([band], [x], [y]) for
        valid data points in source raster src_r
      points_start_index: start index for subset of points to be processed.
      num_points: number of points to process from the start index.
      data_points: dictionary of data point key:value dicts
        into which processed points are  added
      config: dictionary of configuration parameter:values.
      output_csv: output csv file if data_points dict is to be saved in a csv.
      counter: dictionary of named counter:values
    Returns
      True if the raster was processed.
    '''
    logging.info(
        f'Processing points {num_points} of {len(points_index[0])} from {points_start_index} from {src_r.files} ...'
    )
    if counter is None:
        counter = {}
    _set_counter(counter, 'geotiff_width', src_r.shape[1])
    _set_counter(counter, 'geotiff_height', src_r.shape[0])
    _set_counter(counter, 'file_input_points', len(points_index[0]))
    _set_counter(counter, 'file_points_processed', 0)
    _set_counter(counter, 'index_data_points', num_points)
    data_filter = config.get('input_data_filter', None)
    # Get Lat/Lng coordinates for all given x,y points
    # in a batch to reduce number of calls into rasterio.
    latlons = rasterio.transform.xy(src_r.transform, points_index[1],
                                    points_index[2])
    logging.info(f'Got Lat/Lngs for {len(latlons[0])} raster points.')
    # Go over all data points in range.
    num_points = min(num_points, config.get('limit_points', num_points))
    for point_index in range(points_start_index,
                             points_start_index + num_points):
        band = points_index[0][point_index]
        x = points_index[1][point_index]
        y = points_index[2][point_index]
        lng = latlons[0][point_index]
        lat = latlons[1][point_index]
        data = get_raster_data_point(raster=src_r,
                                     arr=src_arr,
                                     x=x,
                                     y=y,
                                     lat=lat,
                                     lng=lng,
                                     config=config,
                                     counter=counter)
        if data:
            # Check if data point is allowed by the ignore/allow masks.
            if ignore_arr is not None:
                ignore_point = get_raster_data_latlng(ignore_r, ignore_arr,
                                                      data['latitude'],
                                                      data['longitude'])
                if ignore_point:
                    logging.debug(
                        f'Ignoring {data} due to ignore mask: {ignore_point}')
                    _add_counter(counter, 'ignored_points', 1)
                    continue
            if allow_arr is not None:
                allow_point = get_raster_data_latlng(allow_r, allow_arr,
                                                     data['latitude'],
                                                     data['longitude'])
                if not allow_point:
                    logging.debug(
                        f'Ignoring {data} due to allow mask: {allow_point}')
                    _add_counter(counter, 'not_allowed_points', 1)
                    continue
            if add_data_point(data_points, data, config, data_filter, counter):
                _add_counter(counter, 'output_data_points', 1)
            _show_counters(counter, config)
    _show_counters(counter)
    if output_csv:
        write_data_csv(data_points, output_csv, [], config, counter)
    return True


def _estimate_num_rows(filename: str) -> int:
    '''Returns the estimated number of rows based on size of the first few rows.'''
    filesize = os.path.getsize(filename)
    with open(filename) as fp:
        lines = fp.read(4000)
    line_size = len(lines) / (lines.count('\n') + 1)
    return filesize / line_size


def process_csv_points(input_csv: str,
                       ignore_csv: str = None,
                       allow_csv: str = None,
                       data_points: dict = None,
                       config: dict = {},
                       output_csv: str = None,
                       counter: dict = None) -> bool:
    '''Process data points from CSV.
    Args:
      input_csv: Input csv file to load data points.
         Assumed to have columns: latitude, longitude
      ignore_csv: CSV file with points to be ignored.
      allow_csv: CSV file with points to be allowed.
      data_points: Dictionary of data points keyed by location+date.
        into which new points from input_csv are added.
      config: dictionary of configuration parameter:values.
      output_csv: Output csv file if data is to be saved.
      counter: dictionary of named counter:values.
    Returns:
      True if data was processed.
   '''
    if data_points is None:
        data_points = {}
    if counter is None:
        counter = {}
    ignore_points = {}
    if ignore_csv:
        filter_config = dict(config)
        filter_config['ignore_date'] = True
        ignore_points = process_csv_points(input_csv=ignore_csv,
                                           config=filter_config)
        logging.info(
            f'Loaded {len(ignore_points)} ignore points from {ignore_csv}')
    allow_points = {}
    if allow_csv:
        filter_config = dict(config)
        filter_config['ignore_date'] = True
        allow_points = process_csv_points(input_csv=allow_csv,
                                          config=filter_config)
        logging.info(
            f'Loaded {len(allow_points)} allow points from {allow_csv}')
    data_filter = config.get('input_data_filter', None)
    input_csv_files = _get_files(input_csv)
    logging.info(f'Processing csv files: {input_csv_files}')
    for filename in input_csv_files:
        _add_counter(counter, 'input_csv_files', 1)
        _set_counter(counter, 'file_input_points', _estimate_num_rows(filename))
        _set_counter(counter, 'file_points_processed', 0)
        with open(filename) as csvfile:
            logging.info(f'Processing data from file {filename} ...')
            reader = csv.DictReader(csvfile)
            # Save the input columns
            config['input_columns'] = reader.fieldnames
            # Process required number of input rows.
            num_rows = 0
            max_rows = config.get('limit_points', sys.maxsize)
            for row in reader:
                num_rows += 1
                if num_rows > max_rows:
                    break
                _add_counter(counter, 'csv_input_points', 1)
                logging.debug(f'Processing CSV row {filename}:{num_rows}:{row}')
                data = get_csv_data_point(row, config)
                if not data:
                  _add_counter(counter, 'csv_input_points_invalid', 1)
                  continue
                data_key = _get_data_key(data, add_date=False)
                if data_key in ignore_points:
                    logging.debug(
                        f'Ignoring point {data} in ignore list: {ignore_points[data_key]}'
                    )
                    _add_counter(counter, 'csv_points_ignored', 1)
                if allow_points and data_key not in allow_points:
                    logging.debug(f'Ignoring point {data} not in allow list')
                    _add_counter(counter, 'csv_points_not_allowed', 1)
                if add_data_point(data_points, data, config, data_filter,
                                  counter):
                    _add_counter(counter, 'csv_points_added', 1)
                _show_counters(counter, config)
            logging.info(f'Processed {num_rows} points from {filename}')
            _set_counter(counter, f'file-points:{os.path.basename(filename)}',
                         num_rows)
            _show_counters(counter)
    return data_points


def filter_data_points(data_points: dict,
                       filter_params: dict = None,
                       config: dict = {},
                       counter: dict = {}) -> dict:
    '''Remove data points that don't meet required criteria, like min area.
    Args:
      data_points: dictionary of data points each with a dict of key:values.
      filter_params: parameters for keys of the data points.
        see is_valid_data_point() for parameters supported.
      config: dictionary of configuration parameter:values.
      output_csv: Output csv file if data is to be saved.
      counter: dictionary of named counter:values.
    Returns:
      data points dictionary after removing points.
    '''
    if not filter_params:
        return data_points
    logging.info(
        f'Filtering {len(data_points)} with params: {filter_params} ...')
    for k in list(data_points.keys()):
        data = data_points[k]
        if not is_valid_data_point(data, filter_params, counter):
            _add_counter(counter, 'data-dropped-filter', 1)
            logging.debug(f'Dropping data {data}')
            data_points.pop(k)
    return data_points


def process(input_geotiff: str,
            input_csv: str,
            output_csv: str,
            config: dict,
            counter: dict = None):
    '''Process raster or CSV inputs to generate CSV output.
    Args:
      input_geotiff: comma separated list of input geotiff file patterns
      input_csv: comma separated list of input csv files.
      output_csv: output csv file to generate
      config: dictionary of config parameter:values.
    '''
    data_points = {}
    if counter is None:
        counter = {}
    _set_counter(counter, 'start_time', time.perf_counter())
    if input_geotiff:
        logging.info(f'Processing raster {input_geotiff}')
        for geotiff_file in _get_files(input_geotiff):
            process_raster(geotiff_file, config, data_points, counter)
    if input_csv:
        logging.info(f'Processing csv {input_csv}')
        process_csv_points(input_csv=input_csv,
                           ignore_csv=config.get('ignore_csv', None),
                           allow_csv=config.get('allow_csv', None),
                           data_points=data_points,
                           config=config,
                           counter=counter)
        logging.info(f'Loaded {len(data_points)} points from {input_csv}')
    _set_counter(counter, 'end_time', time.perf_counter())
    _set_rate_counter(counter, config)

    filter_data_points(data_points, config.get('output_data_filter', None),
                       config, counter)
    write_data_csv(data_points, output_csv, [], config, counter)
    s2_place_file = config.get('output_s2_place', None)
    if s2_place_file:
        write_s2place_csv_tmcf(data_points, s2_place_file, config, counter)
    _show_counters(counter)


def main(_):
    config = load_config(_FLAGS.config)
    if config.get('debug', False):
        logging.set_verbosity(2)
    process(_FLAGS.input_geotiff, _FLAGS.input_csv, _FLAGS.output_csv, config)


if __name__ == '__main__':
    app.run(main)
