## Contamination measurements at the Tar Creek Superfund site
[Tar Creek](https://en.wikipedia.org/wiki/Tar_Creek_Superfund_site) is a superfund site located in the Ottawa County, in northeastern Oklahoma, US. The site has groundwater contaminated as a result of mining activities. 

The data on the level of contamination at the site is measured through several sampling wells that are situated within and around the site in the cities of [Picher](https://en.wikipedia.org/wiki/Picher,_Oklahoma) and [Cardin](https://en.wikipedia.org/wiki/Cardin,_Oklahoma). The measurement of contamination levels are published in the 5-year review report mandated by the EPA to monitor the clean up in superfund sites.

#### How to get the dataset:
For Tar Creek, the 5-year review reports are available as PDF reports at [cumulis.epa.gov](https://cumulis.epa.gov/supercpad/SiteProfiles/index.cfm?fuseaction=second.scs&id=0601269&doc=Y&colid=33990&region=06&type=SC). The [Oklahoma Department of Environmental Quality](https://applications.deq.ok.gov/superfundweb/default.aspx?epaid=OKD980629844) page for Tar Creek has the same reports but are not preferred since the tables are encoded as images which are difficult to extract and process.

We use [`tabula-py`](https://github.com/chezou/tabula-py) to extract tabular data from the PDF files by specifying the page numbers of interest and setting two boolean flags namely: `stream` and `guess` to `True`. This is taken care by the [`process_report2020.py`](process_report2020.py) script.

To simplify future look-up to reproduce the extracted data, please use the following table.

**NOTE:** The links used for the documents may break and it is advisable to download the PDFs from [cumulis.epa.gov](https://cumulis.epa.gov/supercpad/SiteProfiles/index.cfm?fuseaction=second.scs&id=0601269&doc=Y&colid=33990&region=06&type=SC).

|Document Name|Published Year|Pages|Notes|Status|
|-------------|--------------|-----|-----|------|
|[First five-year report of Tar Creek Superfund site](https://semspub.epa.gov/src/document/06/1005834)|1994|140-148|THe formatting is not consistent, needs more curation|TODO|
|[Second five-year report of Tar Creek Superfund site](https://semspub.epa.gov/src/document/06/9291408)|2000|NA|The report has a textual summary of contamination levels|TODO|
|[Third five-year report of Tar Creek Superfund site](https://semspub.epa.gov/src/document/06/196969)|2005|71-74||TODO|
|[Fourth five-year report of Tar Creek Superfund site](https://semspub.epa.gov/src/document/06/694685)|2010|110-115||TODO|
|[Fifth five-year report of Tar Creek Superfund site](https://semspub.epa.gov/src/document/06/9679184)|2015|38-55||TODO|
|[Sixth five-year report of Tar Creek Superfund site](https://semspub.epa.gov/src/document/06/100021610)|2020|83-86||Imported|

> Since the format of the reports are inconsistent across different superfund site and review years, it is advisable to write custom scripts for each report to extract the tabular data. 

#### Extract and process tabular data from PDF reports
The [`process_report2020.py`](process_report2020.py) script is used to extract the tabular data published in the Sixth five-year report of Tar Creek Superfund site, 2020. The script takes as an input a PDF file, which is the file format in which the five-year review reports are available and uses [`tabula-py`](https://github.com/chezou/tabula-py) to extract the tabular data in the specified page number range. 

The [`process_report2020.py`](process_report2020.py) script can be run as

```bash
python3 process_report2020.py --input_path=<path to the PDF on local computer> --pages=<83-89|83,..,89|83>
```
**NOTE:** The intermediate csv file generated is stored in the same location as the location of the PDF file.

>**TIP:** While the script supports the use of URL instead of a local path to the PDF file, it is recommended to download the reports locally and process them.

This script can be used as an example and not as a template for extracting tables from older five-year review reports of Tar Creek. The following are some important points to keep in my mind:
1. Check for the extracted table for decoding errors of values. Eg: We noticed `U` was decoded for a numeric value `11` which needs to be fixed with the `replace()` method in the script
2. Check for malformed columns in the intermediate csv file caught using the [`dc-import`](https://github.com/datacommonsorg/import) tool when validating the generated clean csv and tmcf files generated by [`process_contaminants.py`](process_contaminants.py)

#### Script to generate the clean csv + tmcf
For adding Statistical Variable Observations on the contamination levels across the observed dates in the five-year review report, the [`process_contaminants.py`](process_contaminants.py) script needs to be run with the following arguments,

```bash
python3 process_contaminants.py --input_csv=<path to the intermediate csv file> --output_path=<path to the directory where generated files are saved>
```
