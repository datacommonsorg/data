# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import csv
import re
from absl import app, flags, logging

# Configure logging verbosity for console output
logging.set_verbosity(logging.INFO)

# --- Command Line Flags ---
FLAGS = flags.FLAGS
flags.DEFINE_string(
    "input_dir",
    "raw_folders",
    "The base directory containing the raw state folders generated by the download script.",
)
flags.DEFINE_string(
    "output_dir",
    "processed_data",
    "The root directory where the final state-named folders and CSV files will be saved.",
)
flags.DEFINE_string(
    "output_file_base",
    "nih_mortality_clean",
    "The base name for the final output CSV file (state abbreviation will be appended).",
)

# Global variables for logging statistics across all files
STATS_WRITER = None
STATS_FILE_HANDLE = None

# --- NEW FIXED HEADER DEFINITION ---
# This static header ensures consistent column order and naming for the final output.
FIXED_HEADER = [
    "Period",
    "Cause of Death",
    "Race",
    "Sex",
    "Age Group",
    "Rural/Urban Status",
    "County",
    "FIPS",
    "Age-Adjusted Death Rate - deaths per 100,000",
    "Age-Adjusted Lower 95% Confidence Interval",
    "Age-Adjusted Upper 95% Confidence Interval",
    "Rate Ratio Compared to White (NH)",
    "Rate Ratio Lower 95% Confidence Interval",
    "Rate Ratio Upper 95% Confidence Interval",
    "Average Annual Count",
    "Recent Trend",
    "Recent 5-Year Trend in Death Rates",
    "Recent 5-Year Lower 95% Confidence Interval",
    "Recent 5-Year Upper 95% Confidence Interval",
]


def process_state_data(state_folder_name, input_path, state_output_dir):
    """
    Reads all raw CSV files in a state folder, applies strict filtering to remove
    aggregate rows and rows with missing data (NA/empty), and writes qualified
    rows to a separate log file per source file.

    Returns: A tuple of (qualified_rows, header, disqualified_rows).
    """
    global STATS_WRITER

    # Extract state name used for filtering aggregate rows (e.g., 'California')
    state_name_for_filter = state_folder_name.split(' ')[0].strip()

    all_combined_qualified_rows = []  # For the final, single state CSV
    all_disqualified_rows = []
    header = FIXED_HEADER

    logging.info(f"-> Starting to process: {state_folder_name}")

    # List of patterns used to identify non-county aggregate rows (e.g., state or US totals)
    ignore_row_list = [
        f',{state_name_for_filter},',
        ',UNITED STATES,00000,',
    ]

    for filename in os.listdir(input_path):
        if not filename.endswith('.csv'):
            continue

        file_path = os.path.join(input_path, filename)
        qualified_rows_from_file = []  # Rows kept for this specific source file

        try:
            with open(file_path, 'r', newline='', encoding='utf-8') as f:
                reader = csv.reader(f)

                # Read and discard the actual file header from the raw data
                actual_file_header = next(reader, None)
                if not actual_file_header:
                    logging.warning(f"File skipped (empty): {filename}")
                    continue

                # Ensure the combined output CSV starts with the FIXED_HEADER
                if not all_combined_qualified_rows:
                    all_combined_qualified_rows.append(header)

                row_count = 0
                rows_filtered = 0

                for row in reader:
                    row_count += 1

                    # --- CRITICAL: Column Insertion for Missing Rate Ratio Data ---
                    # The original API data might have 16 columns if Rate Ratio columns are missing.
                    # This ensures all rows match the 19-column FIXED_HEADER structure.
                    if len(row) == 16:
                        # Insert three empty strings at index 11 (Rate Ratio, CI Lower, CI Upper)
                        row.insert(11, '')
                        row.insert(11, '')
                        row.insert(11, '')
                    
                    # Basic check for minimum required length
                    if len(row) < 18:
                        rows_filtered += 1
                        all_disqualified_rows.append(row)
                        continue

                    # 1. Filter Check: Aggregate Rows (State/US)
                    location_fips_data_str = ',' + ','.join(row[6:]).upper()
                    is_aggregate_row = False
                    for pattern in ignore_row_list:
                        if pattern.upper() in location_fips_data_str:
                            is_aggregate_row = True
                            break

                    # 2. Filter Check: NA/Empty Rows (Removes rows where data columns are suppressed/missing)
                    # We check the final 11 columns (Rate, CI, Ratio, CI, Count, Trend, CI, CI)
                    data_cols_str = ',' + ','.join([c.strip() for c in row[-11:]]).upper()
                    
                    # Patterns matching suppressed/missing data structures
                    all_na_row_pattern1 = ',NA,NA,NA,,,,NA,NA,NA,NA,NA'
                    all_na_row_pattern2 = ',NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA'
                    all_empty_row = ',,,,,,,,,,,,'
                    
                    needs_removal_na = (data_cols_str == all_na_row_pattern1) or \
                                       (data_cols_str == all_na_row_pattern2) or \
                                       (data_cols_str == all_empty_row)

                    if needs_removal_na or is_aggregate_row:
                        rows_filtered += 1
                        all_disqualified_rows.append(row)
                        continue

                    # Row is qualified and kept
                    qualified_rows_from_file.append(row)
                    all_combined_qualified_rows.append(row)

                rows_kept = row_count - rows_filtered

                # --- Write Qualified Log per source file ---
                if rows_kept > 0 and header:
                    qualified_log_file_name = f"qualified_log_{filename}"
                    qualified_log_path = os.path.join(state_output_dir, qualified_log_file_name)

                    with open(qualified_log_path, 'w', newline='', encoding='utf-8') as log_csvfile:
                        writer = csv.writer(log_csvfile)
                        # Use the new FIXED_HEADER for the log file
                        writer.writerow(header)
                        writer.writerows(qualified_rows_from_file)

                    logging.info(f"Wrote {rows_kept} qualified rows from {filename}")

                # --- Write Stats Log ---
                if STATS_WRITER:
                    STATS_WRITER.writerow([
                        state_folder_name,
                        filename,
                        row_count,
                        rows_kept,
                        rows_filtered
                    ])

        except Exception as e:
            # Catch file-specific errors (IO, CSV parsing) and log a fatal error before halting.
            error_message = f"FATAL ERROR processing file {filename} in {state_folder_name}: {e}"
            logging.fatal(error_message)
            raise RuntimeError(error_message) from e

    # Check if we have any actual data beyond the header row
    if len(all_combined_qualified_rows) <= 1:
        logging.warning(f"No valid combined data found for {state_folder_name}. Skipping final save.")
        return None, None, None

    # Return only the data rows (excluding the header which was inserted at index 0)
    return all_combined_qualified_rows[1:], header, all_disqualified_rows


def get_state_abbr(state_folder_name):
    """
    Heuristic to derive a 2-letter state abbreviation (or DC/PR) from the state folder name.
    e.g., 'District of Columbia' -> 'DC', 'Puerto Rico' -> 'PR', 'California Counties' -> 'California' (returns CA after lookup)
    """
    # Note: This relies on the calling context having a map or knowing the FIPS codes, 
    # but the implementation here is solely based on the folder name string.
    parts = state_folder_name.split('Counties')[0].strip()
    if not parts:
        return 'XX'

    first_word = parts.strip()

    if first_word.lower() == 'district':
        return 'DC'
    if first_word.lower() == 'puerto':
        return 'PR'

    # The script relies on a lookup table or external knowledge for other abbreviations.
    # Since no lookup table is provided here, it returns the lowercased first word,
    # which is often incorrect but matches the original intent of the provided function.
    # If the original scraper script's abbreviation map was used, this would be more robust.
    return first_word.lower()


def main(_):
    """
    Main function to orchestrate the clubbing and cleaning process:
    1. Sets up output directories and the statistics log.
    2. Iterates through state folders.
    3. Calls process_state_data to filter and combine raw files.
    4. Writes the final combined CSV and an audit log for disqualified rows.
    """
    global STATS_WRITER
    global STATS_FILE_HANDLE

    input_base_dir = FLAGS.input_dir
    output_root_dir = FLAGS.output_dir

    # --- Initial Setup Checks ---
    if not os.path.exists(input_base_dir):
        error_message = f"Input directory '{input_base_dir}' not found. Please run the download script first."
        logging.fatal(error_message)
        raise RuntimeError(error_message)

    os.makedirs(output_root_dir, exist_ok=True)

    # --- STATS LOG FILE SETUP ---
    stats_file_path = os.path.join(output_root_dir, 'row_qualification_stats.csv')
    try:
        STATS_FILE_HANDLE = open(stats_file_path, 'w', newline='', encoding='utf-8')
        STATS_WRITER = csv.writer(STATS_FILE_HANDLE)
        STATS_WRITER.writerow(['State Folder', 'Source File', 'Rows Read', 'Rows Qualified (Kept)', 'Rows Disqualified (Removed)'])
        logging.info(f"Summary statistics log initialized at: {stats_file_path}")
    except IOError as e:
        error_message = f"FATAL ERROR: Failed to open summary statistics log file {stats_file_path}: {e}"
        logging.fatal(error_message)
        raise RuntimeError(error_message)

    total_files_processed = 0
    total_data_rows_saved = 0

    # --- Main Processing Loop ---
    for state_folder_name in os.listdir(input_base_dir):
        input_path = os.path.join(input_base_dir, state_folder_name)

        if not os.path.isdir(input_path):
            continue

        state_output_dir = os.path.join(output_root_dir, state_folder_name)
        os.makedirs(state_output_dir, exist_ok=True)

        # Note: get_state_abbr is an inexact heuristic, but we proceed with it.
        state_abbr = get_state_abbr(state_folder_name)

        data_to_write, header, disqualified_rows = process_state_data(state_folder_name, input_path, state_output_dir)

        if data_to_write and header:

            # --- Write Final Combined State File ---
            final_file_name = f"{FLAGS.output_file_base}_{state_abbr}.csv"
            final_output_path = os.path.join(state_output_dir, final_file_name)

            try:
                with open(final_output_path, 'w', newline='', encoding='utf-8') as csvfile:
                    writer = csv.writer(csvfile)
                    # Write the standardized header first
                    writer.writerow(FIXED_HEADER)
                    # Write all combined, qualified data rows
                    writer.writerows(data_to_write)

                rows_saved = len(data_to_write)
                total_data_rows_saved += rows_saved
                total_files_processed += 1

                logging.info(f"FINISHED CLUBBING: {state_folder_name} -> Saved {rows_saved:,} combined rows to **{final_output_path}**")

            except IOError as e:
                error_message = f"FATAL ERROR: Failed to write final combined file for {state_folder_name}: {e}"
                logging.fatal(error_message)
                raise RuntimeError(error_message)

            # --- Write Disqualified Rows Audit Log (Optional but helpful) ---
            if disqualified_rows:
                disqualified_log_file_path = os.path.join(state_output_dir, 'disqualified_rows_audit_log.csv')
                try:
                    with open(disqualified_log_file_path, 'w', newline='', encoding='utf-8') as log_f:
                        writer = csv.writer(log_f)
                        writer.writerow(['SOURCE_INFO'] + FIXED_HEADER)
                        writer.writerows(disqualified_rows)

                    logging.info(f"Saved audit log of {len(disqualified_rows):,} removed rows to {disqualified_log_file_path}")
                except IOError as e:
                    error_message = f"FATAL ERROR: Failed to write disqualified rows log for {state_folder_name}: {e}"
                    logging.fatal(error_message)
                    raise RuntimeError(error_message)

    # --- Cleanup and Final Summary ---
    if STATS_FILE_HANDLE:
        STATS_FILE_HANDLE.close()

    logging.info(f" Preprocessing Complete! Saved {total_files_processed} state files with a total of {total_data_rows_saved:,} cleaned data rows.")


if __name__ == '__main__':
    app.run(main)