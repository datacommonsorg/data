# IPEDS Admissions and Enrollment National

## Import Overview

This project processes and imports national-level applications, admissions, and enrollment data from the Integrated Postsecondary Education Data System (IPEDS).

*   **Import Name**: IPEDS_Admissions_Enrollment_National
*   **Source URL**: https://nces.ed.gov/ipeds/datacenter/
*   **Provenance Description**: Data on student applications, admissions, and enrollment for postsecondary institutions across the United States.
*   **Import Type**: Automated
*   **Source Data Availability**: Data covers academic years from 2014-15 to 2023-24.
*   **Release Frequency**: Annual

---

## Preprocessing Steps

The import process involves downloading raw data, preprocessing it to remove descriptive rows, and then generating the final artifacts for ingestion.

*   **Input files**:
    *   `download_config.json`: Configuration file for `download_script.py`.
    *   `download_script.py`: Script to download raw data from IPEDS and convert XLSX to CSV.
    *   `input_files/college_admissions_YYYY.csv`: The raw data file for a given year (e.g., `college_admissions_2014.csv`), generated by `download_script.py`.
    *   `college_admissions_ipeds_metadata.csv`: Configuration file for the data processing script.
    *   `pv_map/college_admissions_ipeds_pv_map_YYYY.csv`: Property-value mapping file for a given year.
    *   `admissions_stat_vars_common.mcf`: Common Statistical Variable definitions.

*   **Transformation pipeline**:
    1.  The `download_script.py` is executed to download the raw data from the source, convert it from XLSX to CSV, and place the cleaned CSVs into the `input_files/` directory.
    2.  The `stat_var_processor.py` tool is run for each year's data file, as specified in `admissions_manifest.json`.
    3.  The processor uses the `college_admissions_ipeds_metadata.csv`, the corresponding year's `pv_map/*.csv`, and the `admissions_stat_vars_common.mcf` to generate the final artifacts.
    4.  The output files (`.csv`, `.tmcf`, and `.mcf` files) are placed in the `output_files/` directory.

*   **Data Quality Checks**:
    *   Linting is performed on the generated output files using the Data Commons import tool.
    *   The `dc_generated/report.json` file contains a summary of validation checks, including warnings about year-over-year data fluctuations.

---

## Autorefresh

This import is considered automated due to the inclusion of `download_script.py` in the pipeline.

*   **Steps**:
    1.  Execute `download_script.py` to fetch the raw data files into `input_files/`.
    2.  The `stat_var_processor.py` tool is then run (as defined in `admissions_manifest.json`) on the preprocessed files to generate the final artifacts for ingestion.
    3.  A corresponding `college_admissions_ipeds_pv_map_YYYY.csv` file should be available in the `pv_map/` directory for each year.

---

## Script Execution Details

To run the import manually, follow these steps.

### Step 1: Download and Preprocess Raw Data (via `download_script.py`)

This script downloads the raw data from the IPEDS website, converts it from XLSX to CSV, and places the cleaned data in the `input_files/` directory. It uses `download_config.json` for URL and filename information.

**Usage**:

```shell
python3 download_script.py
```

---

### Step 2: Process the Data for Final Output

This step involves running the `stat_var_processor.py` for each input file as specified in `admissions_manifest.json`. An example command for the 2014 data is shown below:

**Usage**:

```shell
python3 ../../tools/statvar_importer/stat_var_processor.py --existing_statvar_mcf=admissions_stat_vars_common.mcf --input_data=input_files/college_admissions_2014.csv --pv_map=pv_map/college_admissions_ipeds_pv_map_2014.csv --config_file=college_admissions_ipeds_metadata.csv --output_path=output_files/admissions_output_2014
```

_Note: This command needs to be executed for all input files as defined in `admissions_manifest.json`._

---

### Step 3: Validate the Output Files

This command validates the generated files for formatting and semantic consistency before ingestion.

**Usage**:

```shell
java -jar /path/to/datacommons-import-tool.jar lint -d 'output_files/'
```

This step ensures that the generated artifacts are ready for ingestion into Data Commons.
